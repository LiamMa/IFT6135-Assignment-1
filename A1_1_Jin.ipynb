{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1_1_Jin.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "ajrMnZLJeMHv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "np.random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IgyWZVSD64lB",
        "colab_type": "code",
        "outputId": "6a789ade-c022-4fdb-9a63-ec334411aafe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HN603fuc-9PX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DATA_PATH = r'/content/gdrive/My Drive/app/MNIST'\n",
        "X_train = np.load(DATA_PATH + '/x_train.npy')\n",
        "y_train = one_hot(np.load(DATA_PATH + '/y_train.npy'),10)\n",
        "X_val   = np.load(DATA_PATH + '/x_val.npy')\n",
        "y_val   = one_hot(np.load(DATA_PATH + '/y_val.npy'),10)\n",
        "X_test  = np.load(DATA_PATH + '/x_test.npy')\n",
        "y_test  = one_hot(np.load(DATA_PATH + '/y_test.npy'),10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BXmeo9zw7Swx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def one_hot(labels, n):\n",
        "    m = len(labels)\n",
        "    onehot = np.zeros((m, n))\n",
        "    onehot[np.arange(m), labels] = 1\n",
        "    return onehot\n",
        "\n",
        "def accuracy(y_pred, y):\n",
        "    return np.sum(np.argmax(y_pred, axis=1) == np.argmax(y, axis=1)) / y.shape[0]\n",
        "\n",
        "\n",
        "def data_iter(data, batch_size):\n",
        "    X, y = data\n",
        "    batches = [(X[i:i+batch_size],y[i:i+batch_size]) for i in range(0,X.shape[0],batch_size)]\n",
        "    random.shuffle(batches)\n",
        "    for batch in batches:\n",
        "        yield batch\n",
        "                \n",
        "        \n",
        "def glorot(in_dim, out_dim):\n",
        "    d = np.sqrt(6/(in_dim+out_dim))\n",
        "    return np.random.normal(-d,d,(in_dim,out_dim))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UONENIM-fV0l",
        "colab_type": "code",
        "outputId": "3d5cc421-da3b-46d4-f698-c49f1722f131",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "INPUT_DIM = 784\n",
        "OUTPUT_DIM = 10\n",
        "\n",
        "class NN(object):\n",
        "    \n",
        "    \n",
        "    def __init__(self,hidden_dims=(1024,2048),n_hidden=2,init_method='normal',activate='relu',mode='train'):\n",
        "        self.dims = [INPUT_DIM,] + hidden_dims + [OUTPUT_DIM,]\n",
        "        self.weights = []\n",
        "        self.biases  = []\n",
        "        self.init = init_method\n",
        "        self.activate = activate\n",
        "        \n",
        "        initialize_weights(n_hidden, self.dims)\n",
        "        \n",
        "        \n",
        "    def initialize_weights(self, n_hidden, dims):\n",
        "        if self.init == 'Zero':\n",
        "            init_method = lambda x, y: np.zeros(x,y)\n",
        "        elif self.init == 'Normal':\n",
        "            init_method = lambda x, y: np.random.randn(x,y)\n",
        "        elif self.init == 'Glorot':\n",
        "            init_method = glorot\n",
        "            \n",
        "        for (inputs, outputs) in zip(dims[:-1], dims[1:]):\n",
        "            self.weights.append(init_method(inputs, outputs))\n",
        "            self.biases.append(np.zeros(outputs))\n",
        "            \n",
        "            \n",
        "    def activation(self,inputs):\n",
        "        if self.activate == 'relu':\n",
        "            return np.max(inputs,0)\n",
        "        if self.activate == 'sigmoid':\n",
        "            return 1.0/(1.0+np.exp(-inputs))\n",
        "            \n",
        "            \n",
        "    def forward(self, inputs):\n",
        "        a_k = None\n",
        "        h_k = inputs\n",
        "        a = []\n",
        "        h = []\n",
        "        for (W, b) in zip(self.weights[:-1], self.biases[:-1]):\n",
        "            a_k = np.dot(h_k, W)\n",
        "            h_k = activation(a_k)\n",
        "            a.append(a_k)\n",
        "            h.append(h_k)\n",
        "        \n",
        "        a_k = np.dot(h_k, self.weights[-1]) + self.biases[-1]\n",
        "        h_k = softmax(a_k)\n",
        "        a.append(a_k)\n",
        "        h.append(h_k)\n",
        "        cache = {'a':a, 'h':h}\n",
        "        \n",
        "        return h_k, cache\n",
        "    \n",
        "    \n",
        "    \n",
        "    def loss(self, pred, labels):\n",
        "        '''\n",
        "        Negative log likelihood\n",
        "        '''\n",
        "        ls = np.log(pred)\n",
        "        ls = - np.sum(ls * labels)\n",
        "        return ls\n",
        "    \n",
        "    \n",
        "    def backward(self,cache,labels,...):\n",
        "        \n",
        "        \n",
        "        \n",
        "    def update(self,grads,lr):\n",
        "        grads_w = grads['w']\n",
        "        grads_b = grads['b']\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] -= lr * grads_w[i]\n",
        "            self.biases[i] -= lr * grads_b[i]\n",
        "            \n",
        "\n",
        "    def train(self, data, epochs, batch_size, lr, lambd=0.0, test_data=None):\n",
        "        for ep in range(1, epochs+1):\n",
        "            for (batch_x, batch_y) in data_iter(data, batch_size):\n",
        "                \n",
        "                y_pred, cache = self.forward(batch_x, batch_y)\n",
        "                grads = self.backward(cache)\n",
        "                self.update(grads, lr)\n",
        "\n",
        "                \n",
        "    def test(self, data):\n",
        "        x, y = data\n",
        "        outputs, _ = forward(x)\n",
        "        outputs = onehot_reverse(outputs)\n",
        "        acc = np.sum(outputs == y) / len(data)\n",
        "        \n",
        "    \n",
        "    def activate_grad(inputs):\n",
        "        if self.activate == 'relu':\n",
        "            inputs[inputs > 0] = 1\n",
        "            inputs[inputs < 0] = 0\n",
        "        elif self.activate == 'sigmiod':\n",
        "            return activation(inputs) * (1 - activation(inputs))\n",
        "        \n",
        "        \n",
        "    def softmax(self,inputs):\n",
        "        outputs = np.exp(inputs)\n",
        "        return outputs / (np.sum(outputs, axis=1))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "ULc4DpLmPJ3L",
        "colab_type": "code",
        "outputId": "a59f4a65-0328-4c20-d1d9-8bf54f557ffb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "a = np.arange(12)\n",
        "a[a > 1] = 1\n",
        "a"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "Q_e1swpLPNFY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}