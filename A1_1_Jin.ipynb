{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1_1_Jin.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "ajrMnZLJeMHv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UONENIM-fV0l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3d5cc421-da3b-46d4-f698-c49f1722f131"
      },
      "cell_type": "code",
      "source": [
        "INPUT_DIM = 784\n",
        "OUTPUT_DIM = 10\n",
        "\n",
        "class NN(object):\n",
        "    \n",
        "    def __init__(self,hidden_dims=(1024,2048),n_hidden=2,init_method='normal',mode='train',datapath=None,model_path=None):\n",
        "        self.dims = [INPUT_DIM,] + hidden_dims + [OUTPUT_DIM]\n",
        "        self.weights = []\n",
        "        self.biases  = []\n",
        "        \n",
        "        initialize_weights(n_hidden, self.dims, init_method)\n",
        "        \n",
        "        \n",
        "    def initialize_weights(self, n_hidden, dims, init_method):\n",
        "        for (input_, output_) in zip(dims[:-1], dims[1:]):\n",
        "            self.weights.append(init_method(input_, output_))\n",
        "            self.biases.append(np.zeros(output_))\n",
        "            \n",
        "    def forward(self,input_,labels):\n",
        "        output_ = input_\n",
        "        for (W, b) in zip(self.weights, self.biases):\n",
        "            output_ = np.dot(output_, W) + b\n",
        "        \n",
        "        output_ = softmax(output_)\n",
        "        output_loss = loss(output_, labels)\n",
        "        \n",
        "        return output_, output_loss\n",
        "    \n",
        "    \n",
        "    def activation(self,input,activate='relu'):\n",
        "        if activate == 'relu':\n",
        "            return np.max(input,0)\n",
        "        \n",
        "    def softmax(self,input):\n",
        "        output_ = np.exp(input)\n",
        "        return output_ / (np.sum(output_, axis=1))\n",
        "    \n",
        "    def loss(self, pred, labels):\n",
        "        ls = np.log(pred)\n",
        "        ls = np.sum(ls * labels)\n",
        "        return ls\n",
        "    \n",
        "    def backward(self,cache,labels,...):\n",
        "        \n",
        "    def update(self,grads,..):\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "    def test(self):\n",
        "        "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    }
  ]
}