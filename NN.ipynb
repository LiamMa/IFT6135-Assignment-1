{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NN.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"KQ8GvZzaXl73","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"f418f228-25c7-4397-a3c4-300fb9ee8856","executionInfo":{"status":"ok","timestamp":1548509770457,"user_tz":300,"elapsed":23837,"user":{"displayName":"Maximilien Le Clei","photoUrl":"https://lh4.googleusercontent.com/-MWdkEHlJfJ8/AAAAAAAAAAI/AAAAAAAAALk/iuE1yhmpMCI/s64/photo.jpg","userId":"17481473454263177289"}}},"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"O38csOzLXl7-","colab_type":"code","colab":{}},"cell_type":"code","source":["def one_hot(old_y, m):\n","    \n","    n = len(old_y)\n","    \n","    y = np.zeros((n, m))\n","    \n","    y[np.arange(n), old_y] = 1\n","    \n","    return y"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KTX-SH2lXl8I","colab_type":"text"},"cell_type":"markdown","source":["## MNIST"]},{"metadata":{"id":"85uur8TaXl8J","colab_type":"code","colab":{}},"cell_type":"code","source":["# ~~ MNIST dataset ~~\n","\n","X_train = np.load('/content/gdrive/My Drive/Datasets/MNIST/x_train.npy')\n","y_train = one_hot(np.load('/content/gdrive/My Drive/Datasets/MNIST/y_train.npy'), 10)\n","\n","X_val = np.load('/content/gdrive/My Drive/Datasets/MNIST/x_val.npy')\n","y_val = one_hot(np.load('/content/gdrive/My Drive/Datasets/MNIST/y_val.npy'), 10)\n","\n","X_test = np.load('/content/gdrive/My Drive/Datasets/MNIST/x_test.npy')\n","y_test = one_hot(np.load('/content/gdrive/My Drive/Datasets/MNIST/y_test.npy'), 10)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"55p4xkXOXl8M","colab_type":"code","colab":{}},"cell_type":"code","source":["class NN:\n","    \n","    # ----- Constructor ----- #\n","        \n","    def __init__(self, hidden_dims, n_hidden, initialization_scheme):\n","        \n","        dims = [784] + hidden_dims + [10]\n","        \n","        self.W = []\n","        self.b = []\n","        \n","        self.n_hidden = n_hidden\n","        \n","        self.m = 10 # Number of classes\n","        \n","        self.initialize_weights(n_hidden, dims, initialization_scheme)\n","\n","        \n","    # ----- Initializations ----- #\n","        \n","    def initialize_weights(self, n_hidden, dims, initialization_scheme):\n","    \n","        # Zero    \n","        if initialization_scheme == 'Zero':\n","            \n","            for i in range(n_hidden + 1):\n","            \n","                # Weights set to 0\n","            \n","                self.W.append( np.zeros(( dims[i+1], dims[i] )) )\n","        \n","                # Biases set to 0\n","            \n","                self.b.append( np.zeros( dims[i+1] ) )\n","        \n","        # Normal        \n","        elif initialization_scheme == 'Normal':\n","            \n","            for i in range(n_hidden + 1):\n","            \n","                # Weights sampled from N(0,1)\n","            \n","                self.W.append( np.random.randn( dims[i+1], dims[i] ) )\n","        \n","                # Biases set to 0\n","            \n","                self.b.append( np.zeros( dims[i+1] ) )\n","        \n","        # Glorot        \n","        else: #initialization_scheme == 'Glorot'\n","            \n","            for i in range(n_hidden + 1):            \n","            \n","                # Weights sampled from U(-d^l, d^l), d^l = sqrt( 6 / h^(l-1) + h^l )\n","            \n","                d = np.sqrt( 6 / (dims[i] + dims[i+1]) )\n","                self.W.append( np.random.uniform( -d, d, (dims[i+1], dims[i]) ) )\n","        \n","                # Biases ~ 0\n","            \n","                self.b.append( np.zeros(dims[i+1]) )\n","\n","                \n","    # ----- ReLU activation ----- #\n","    \n","    def activation(self, inputs):\n","        \n","        zeros = np.zeros_like(inputs)\n","        \n","        return np.maximum(zeros, inputs)\n","   \n","\n","    # ----- Softmax ----- #\n","    \n","    def softmax(self, inputs):\n","        \n","        n, _ = inputs.shape\n","        \n","        # Numerically stable softmax\n","        \n","        b = np.max(inputs, axis=1).reshape((n, 1))\n","        \n","        return np.exp(inputs - b) / np.sum( np.exp(inputs - b), axis=1).reshape((n, 1))\n","        \n","        \n","    # ----- Cross Entropy Loss ----- #\n","    \n","    def loss(self, predictions, labels):\n","        \n","        n, _ = predictions.shape\n","        \n","        losses = np.sum( - np.log(predictions + 0.00001) * labels, axis=1 ) # To avoid log(0)\n","        \n","        return np.sum(losses) / n # Average loss (Empirical risk)\n","        \n","        \n","    # ----- Forward Propagation ----- #\n","    \n","    def forward(self, X, y):\n","    \n","        n, _ = X.shape\n","    \n","        self.a = [X]\n","    \n","        self.h = []\n","    \n","        for i in range(self.n_hidden):\n","            \n","            self.h.append( np.matmul(self.a[i], self.W[i].T) + self.b[i] )\n","            \n","            self.a.append( self.activation(self.h[i])  )\n","            \n","        self.h_output = np.matmul(self.a[-1], self.W[-1].T) + self.b[-1]\n","        \n","        self.a_output = self.softmax(self.h_output)\n","        \n","        y_hat = one_hot(np.argmax(self.a_output, axis=1), self.m)\n","        \n","        return y_hat, self.loss(self.a_output, y)   \n","    \n","    \n","    # ----- Backward Propagation ----- #\n","    \n","    def backward(self, X, y):\n","        \n","        n, d = X.shape\n","        \n","        # Set up list storing gradients\n","\n","        grad_W = []\n","        grad_b = []\n","\n","        for i in range(self.n_hidden + 1):\n","            \n","            grad_W.append(None)\n","            grad_b.append(None)\n","            \n","        for i in range(self.n_hidden, -1, -1):\n","            \n","            # Softmax\n","            if i == self.n_hidden:    \n","                grad_h = np.copy(self.a_output)\n","                grad_h -= y\n","                grad_h /= n\n","            \n","            # ReLU\n","            else: \n","                grad_h = grad_a * ( (self.h[i] > 0) * 1 )\n","            \n","            grad_W[i] = np.matmul(grad_h.T, self.a[i])\n","            \n","            grad_b[i] = np.sum(grad_h, axis=0)\n","            \n","            grad_a = np.matmul(grad_h, self.W[i])\n","        \n","        return (grad_W, grad_b)\n","    \n","        \n","    # ----- Update weights ----- # \n","    \n","    def update(self, grads, eta):\n","        \n","        grad_W, grad_b = grads\n","        \n","        for l in range(self.n_hidden + 1):\n","                    \n","            self.W[l] -= eta * grad_W[l]\n","            self.b[l] -= eta * grad_b[l]\n","\n","            \n","    # ----- Train ----- #\n","    \n","    def train(self, X, y, epochs, eta, K):\n","        \n","        n, d = X.shape\n","        \n","        n, m = y.shape\n","        \n","        # Batch Gradient Descent set up\n","        \n","        nb_batches = int(n / K)\n","        \n","        X_batches = np.zeros((nb_batches, K, d))\n","        y_batches = np.zeros((nb_batches, K, m))\n","        \n","        for i in range(nb_batches):\n","            \n","            batch_indexes = np.linspace(i*K, (i+1)*K - 1, K).astype(int)\n","            \n","            X_batches[i] = X[batch_indexes]\n","            \n","            y_batches[i] = y[batch_indexes]\n","            \n","        print( self.test(X_train, y_train) )\n","            \n","        for i in range(epochs):\n","            \n","            for j in range(nb_batches):\n","                    \n","                X_batch = X_batches[j]\n","                y_batch = y_batches[j]    \n","                \n","                # Forward pass\n","                self.forward(X_batch, y_batch)                 \n","                    \n","                # Backward pass\n","                gradients = self.backward(X_batch, y_batch)\n","                       \n","                # Update parameters\n","                self.update(gradients, eta)\n","                \n","            print( self.test(X_train, y_train) )\n","                \n","    # ----- Test ----- #\n","    \n","    def test(self, X, y):\n","        \n","        n, _ = X.shape\n","        \n","        y_hat, loss = self.forward(X, y)\n","            \n","        accuracy = np.sum( np.argmax(y, axis=1) == np.argmax(y_hat, axis=1)) / n * 100\n","        \n","        return accuracy, loss"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":false,"id":"31eCAeYZXl8O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"a4263db6-29ff-4fad-de0f-f2b53e750bb3","executionInfo":{"status":"ok","timestamp":1548515599019,"user_tz":300,"elapsed":234595,"user":{"displayName":"Maximilien Le Clei","photoUrl":"https://lh4.googleusercontent.com/-MWdkEHlJfJ8/AAAAAAAAAAI/AAAAAAAAALk/iuE1yhmpMCI/s64/photo.jpg","userId":"17481473454263177289"}}},"cell_type":"code","source":["np.random.seed(0)\n","\n","model = NN(hidden_dims=[666, 666], n_hidden=2, initialization_scheme='Zero')\n","model.train(X_train, y_train, 10, 0.001, 100)"],"execution_count":38,"outputs":[{"output_type":"stream","text":["(9.864, 2.302485097993712)\n","(11.356, 2.3023334815422447)\n","(11.356, 2.302196316623517)\n","(11.356, 2.302072252911978)\n","(11.356, 2.301960062306546)\n","(11.356, 2.301858628374674)\n","(11.356, 2.301766936653709)\n","(11.356, 2.3016840657433324)\n","(11.356, 2.301609179128462)\n","(11.356, 2.301541517676853)\n","(11.356, 2.3014803927599443)\n"],"name":"stdout"}]},{"metadata":{"id":"ZxcOtK0ZXl8Q","colab_type":"code","colab":{}},"cell_type":"code","source":["zero = []\n","glorot = []\n","normal = []"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rt23IfDvtKu7","colab_type":"code","colab":{}},"cell_type":"code","source":["from pylab import rcParams\n","rcParams['figure.figsize'] = 15, 10\n","\n","plt.xticks(np.arange(0, 11, step=1))\n","plt.xlabel('Epoch', weight='bold')\n","plt.ylabel('Average Loss on Training Set', weight='bold')\n","plt.title('Empirical Risk - Training Set - MNIST - 3 Different Initialization Methods', weight='bold')\n","\n","plt.plot(np.arange(1, 11, step=1), normal, label='Normal')\n","plt.plot(np.arange(1, 11, step=1), glorot, label='Glorot')\n","plt.plot(np.arange(1, 11, step=1), zero, label='Zero')\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]}]}