{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Feedforward Neural Network.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"NBVBGiqBGaGV","colab_type":"text"},"cell_type":"markdown","source":["# Setup"]},{"metadata":{"id":"1ZmGTwN2Fdvn","colab_type":"code","colab":{}},"cell_type":"code","source":["# For Google Collab\n","\n","# http://pytorch.org/\n","from os.path import exists\n","from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n","accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n","\n","!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aezmD0iyFhbl","colab_type":"code","outputId":"4db0a184-2e1b-472f-fb81-2805cff93de7","executionInfo":{"status":"ok","timestamp":1548109833499,"user_tz":300,"elapsed":2171,"user":{"displayName":"Maximilien Le Clei","photoUrl":"https://lh4.googleusercontent.com/-MWdkEHlJfJ8/AAAAAAAAAAI/AAAAAAAAALk/iuE1yhmpMCI/s64/photo.jpg","userId":"17481473454263177289"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"f8v_2HS_FWok","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eJGlLnz6FWon","colab_type":"code","colab":{}},"cell_type":"code","source":["def one_hot(old_y, m):\n","    \n","    n = len(old_y)\n","    \n","    y = np.zeros((n, m))\n","    \n","    y[np.arange(n), old_y] = 1\n","    \n","    return y"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fyyXtznUFl31","colab_type":"code","colab":{}},"cell_type":"code","source":["# ~~ MNIST dataset ~~\n","\n","X_train = np.load('/content/gdrive/My Drive/Datasets/MNIST/x_train.npy')\n","y_train = one_hot(np.load('/content/gdrive/My Drive/Datasets/MNIST/y_train.npy'), 10)\n","\n","X_val = np.load('/content/gdrive/My Drive/Datasets/MNIST/x_val.npy')\n","y_val = one_hot(np.load('/content/gdrive/My Drive/Datasets/MNIST/y_val.npy'), 10)\n","\n","X_test = np.load('/content/gdrive/My Drive/Datasets/MNIST/x_test.npy')\n","y_test = one_hot(np.load('/content/gdrive/My Drive/Datasets/MNIST/y_test.npy'), 10)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"L7f4RiOeFWoo","colab_type":"text"},"cell_type":"markdown","source":["# Model"]},{"metadata":{"id":"d0DOsVFLFWoy","colab_type":"code","colab":{}},"cell_type":"code","source":["class NN:\n","    \n","    # ReLU activation\n","    \n","    def activation(self, inputs):\n","        \n","        zeros = np.zeros_like(inputs)\n","        \n","        return np.maximum(zeros, inputs)\n","    \n","    # Softmax\n","    \n","    def softmax(self, inputs):\n","        \n","        # Numerically stable softmax\n","        \n","        b = np.max(inputs, axis=1).reshape((n, 1))\n","        \n","        return np.exp(inputs - b) / np.sum( np.exp(inputs - b), axis=1 ).reshape((n, 1))\n","        \n","    # Different types of initialization\n","    \n","    def initialize_weights(self, n_hidden, dims):\n","    \n","        # Zero\n","    \n","        if self.initialization = 'Zero':\n","            \n","            for i in range(n_hidden + 1):\n","            \n","                # Weights set to 0\n","            \n","                self.W.append( np.zeros(( dims[i+1], dims[i] )) )\n","        \n","                # Biases set to 0\n","            \n","                self.b.append( np.zeros( dims[i+1] ) )\n","        \n","        # Normal\n","        \n","        elif self.initialization = 'Normal':\n","            \n","            for i in range(n_hidden + 1):\n","            \n","                # Weights sampled from N(0,1)\n","            \n","                self.W.append( np.random.randn( dims[i+1], dims[i] ) )\n","        \n","                # Biases set to 0\n","            \n","                self.b.append( np.zeros( dims[i+1] ) )\n","        \n","        # Glorot\n","        \n","        else: #self.initialization = 'Glorot'\n","            \n","            for i in range(L + 1):            \n","            \n","                # Weights sampled from U(-d^l, d^l), d^l = sqrt( 6 / h^(l-1) + h^l )\n","            \n","                d = np.sqrt( 6 / dims[i] + dims[i+1] )\n","            \n","                self.W.append( np.random.uniform( -d, d, (dims[i+1], dims[i]) ) )\n","        \n","                # Biases ~ 0\n","            \n","                self.b.append( np.zeros(dimensions[i+1]) )\n","        \n","        \n","    def __init__(self, hidden_dims, initialization):\n","        \n","        \n","    \n","    def __init__(self, L, d, d_h, m, lambdas):\n","        \n","        self.d = d\n","        self.d_h = d_h\n","        self.m = m\n","\n","        self.W = []\n","        self.b = []\n","        \n","        self.L = L\n","        \n","        dims = [d] + d_h + [m]\n","        \n","        self.dimensions = dimensions\n","        \n","        for i in range(L + 1):\n","            \n","            boundary = 1 / np.sqrt(dimensions[i])\n","            \n","            self.W.append( np.random.uniform(-boundary, boundary, (dimensions[i+1], dimensions[i])) )\n","        \n","            # Biases ~ 0\n","            \n","            self.b.append( np.zeros(dimensions[i+1]) )\n","            \n","        # Lambdas - Elastic net (L1 + L2 regularization)\n","            \n","        self.lambdasL1, self.lambdasL2 = lambdas\n","        \n","    \n","    def update(self, grads)\n","    \n","    def train(self, X, y, epochs, eta, K):\n","        \n","        n, d = X.shape\n","        \n","        n, m = y.shape\n","            \n","        # Stochastic Gradient Descent\n","            \n","        for i in range(epochs):\n","                \n","            # Forward pass\n","                \n","            self.forward_propagation(X_batch, y_batch)                 \n","                    \n","            # Backward pass\n","                    \n","            grad_W, grad_b = self.backward_propagation(X_batch, y_batch)\n","                       \n","            # Gradient updates\n","                \n","            for l in range(self.L + 1):\n","                    \n","                self.W[l] -= eta * grad_W[l]\n","                self.b[l] -= eta * grad_b[l]\n","    \n","    def loss(self, predictions, labels):\n","        \n","        \n","    \n","    def forward(self, X, y):\n","    \n","        n, d = X.shape\n","    \n","        self.a = [X]\n","    \n","        self.h = []\n","    \n","        for i in range(self.L):\n","            \n","            self.h.append(np.matmul(self.a[i], self.W[i].T) + self.b[i])\n","            \n","            self.a.append( self.activation(self.h[i])  )\n","            \n","        self.h_output = np.matmul(self.a[-1], self.W[-1].T) + self.b[-1]\n","        \n","        self.a_output = self.softmax(self.h_output)\n","        \n","        y_hat = one_hot(np.argmax(self.a_output, axis=1), self.m)\n","        \n","        L_x = - np.log(self.a_output + 0.00001) # To avoid log(0)\n","        \n","        L_x_y = np.sum(L_x * y, axis=1)\n","        \n","        R = np.sum(L_x_y) / n\n","            \n","        for i in range(self.L + 1):\n","            \n","            R += self.lambdasL1[i] * np.sum(np.abs(self.W[i])) + self.lambdasL2[i] * np.sum(self.W[i] ** 2)\n","    \n","        return y_hat, R\n","    \n","    \n","    def backward(self, X, y):\n","    \n","        n, d = X.shape\n","        \n","        # Set up list storing gradients\n","\n","        grad_W = []\n","        grad_b = []\n","\n","        for i in range(self.L + 1):\n","            \n","            grad_W.append(None)\n","            grad_b.append(None)\n","            \n","        for i in range(self.L, -1, -1):\n","            \n","            # ∇ h_output ~ Softmax\n","            if i == self.L:    \n","                grad_h = self.a_output\n","                grad_h -= y\n","                grad_h /= n\n","            \n","            # ∇ h_hidden ~ ReLU\n","            else: \n","                grad_h = grad_a * ( (self.h[i] > 0) * 1 )\n","            \n","            # ∇ W ~ ∇ W_L -> ∇ W_1\n","            grad_W[i] = np.matmul(grad_h.T, self.a[i]) + self.lambdasL1[i] * np.sign(self.W[i]) + \\\n","                                                            2 * self.lambdasL2[i] * self.W[i]\n","            # ∇ b_i ~ ∇ b_L -> ∇ b_1\n","            grad_b[i] = np.sum(grad_h, axis=0)\n","            \n","            # ∇ a ~ ∇ a_L -> ∇ X \n","            grad_a = np.matmul(grad_h, self.W[i])\n","        \n","        return grad_W, grad_b"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":false,"id":"oM0s8ZnXFWo0","colab_type":"code","outputId":"052526d6-475d-4510-81d3-bf99343419dc","colab":{}},"cell_type":"code","source":["np.random.seed(0)\n","\n","n, d = X_train_fashion.shape\n","n, m = y_train_fashion.shape\n","\n","d_h = []\n","\n","L = len(d_h)\n","\n","lambdasL1 = [0.00002]*(L+1)\n","lambdasL2 = [0.00002]*(L+1)\n","\n","lambdas = [lambdasL1, lambdasL2]\n","\n","model = Feedforward_Neural_Network(L, d, d_h, m, lambdas)\n","\n","model.train(X_train_fashion, y_train_fashion, 30, 0.05, 10)\n","\n","# Train\n","\n","n, _ = X_train_fashion.shape\n","\n","y_hat_train, _ = model.forward_propagation(X_train_fashion, y_train_fashion)\n","     \n","missclassification = np.sum( np.argmax(y_train_fashion, axis=1) != np.argmax(y_hat_train, axis=1)) / n * 100\n","    \n","print(\"Final missclassification on training set: \", missclassification, \"%\")\n","\n","# Validation\n","\n","n, _ = X_val_fashion.shape\n","\n","y_hat_val, _ = model.forward_propagation(X_val_fashion, y_val_fashion)\n","            \n","missclassification = np.sum( np.argmax(y_val_fashion, axis=1) != np.argmax(y_hat_val, axis=1)) / n * 100\n","    \n","print(\"Final missclassification on validation set: \", missclassification, \"%\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 0 , Missclassification: 87.146 %\n","Epoch 1 , Missclassification: 29.396 %\n","Epoch 2 , Missclassification: 30.98 %\n","Epoch 3 , Missclassification: 24.468 %\n","Epoch 4 , Missclassification: 31.044 %\n","Epoch 5 , Missclassification: 21.07 %\n","Epoch 6 , Missclassification: 22.79 %\n","Epoch 7 , Missclassification: 25.15 %\n","Epoch 8 , Missclassification: 29.836000000000002 %\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-556db85e8ac0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeedforward_Neural_Network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_fashion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_fashion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-3fcc39ffa468>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, iterations, eta, K)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-3fcc39ffa468>\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# Numerically stable softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_output\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_output\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"bbcp-gTvFWo3","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}