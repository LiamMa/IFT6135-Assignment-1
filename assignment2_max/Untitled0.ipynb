{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["sFIMUkyM6mNo"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"DDGDosZKHf9j","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch \n","import torch.nn as nn\n","\n","import numpy as np\n","import torch.nn.functional as F\n","import math, copy, time\n","from torch.autograd import Variable\n","import matplotlib.pyplot as plt\n","\n","import argparse\n","import time\n","import collections\n","import os\n","import sys"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8aRwk1Jz5JaK","colab_type":"code","outputId":"4376e82b-a836-4183-a345-b7af78858232","executionInfo":{"status":"ok","timestamp":1552929932925,"user_tz":240,"elapsed":22329,"user":{"displayName":"Maximilien Le Clei","photoUrl":"https://lh4.googleusercontent.com/-MWdkEHlJfJ8/AAAAAAAAAAI/AAAAAAAAALk/iuE1yhmpMCI/s64/photo.jpg","userId":"17481473454263177289"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","path = 'drive/My Drive/IFT6135/A2/'"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"metadata":{"id":"lVzcovbI6gU5","colab_type":"text"},"cell_type":"markdown","source":["# RNN"]},{"metadata":{"id":"Ejb-8BCJ6C7S","colab_type":"code","colab":{}},"cell_type":"code","source":["class RNNCell(nn.Module):\n","\n","\tdef __init__(self, input_size, hidden_size, dp_keep_prob):\n","    \n","\t\tsuper(RNNCell, self).__init__()\n","\n","\t\tself.hidden_size = hidden_size\n","\n","\t\tself.fc_h = nn.Linear(input_size + hidden_size, hidden_size)\n","\n","\t\tself.dropout = nn.Dropout(1 - dp_keep_prob)\n","\t\tself.tanh = nn.Tanh()\n","\n","\n","\tdef init_weights(self):\n","\n","\t\tk = np.sqrt(1 / self.hidden_size)\n","\n","\t\tnn.init.uniform_(self.fc_h.weight, -k, k)\n","\t\tnn.init.uniform_(self.fc_h.bias, -k, k)\n","\n","\tdef forward(self, x, h):\n","    \n","\t\tx = self.dropout(x)\n","\n","\t\tx__h = torch.cat((x, h), dim=1)\n","\n","\t\th = self.tanh( self.fc_h(x__h) )\n","\n","\t\treturn h\t\n","\n","class RNN(nn.Module):\n","\n","\tdef __init__(self, emb_size, hidden_size, seq_len, batch_size, vocab_size, num_layers, dp_keep_prob):\n","\t\t\n","\t\tsuper(RNN, self).__init__()\n","\n","\t\tself.emb_size = emb_size\n","\t\tself.hidden_size = hidden_size\n","\t\tself.seq_len = seq_len\n","\t\tself.batch_size = batch_size\n","\t\tself.vocab_size = vocab_size\n","\t\tself.num_layers = num_layers\n","\t\tself.dp_keep_prob = dp_keep_prob\n","\n","\t\tself.dropout = nn.Dropout(1 - dp_keep_prob)\n","\n","\t\tself.embedding_layer = nn.Embedding(vocab_size, emb_size)\n","\n","\t\tself.hidden_layers = nn.ModuleList()\n","\n","\t\tfor i in range(num_layers):\n","\n","\t\t\tif i == 0:\n","\t\t\t\tself.hidden_layers.append( RNNCell(emb_size, hidden_size, dp_keep_prob) )\n","\t\t\telse:\n","\t\t\t\tself.hidden_layers.append( RNNCell(hidden_size, hidden_size, dp_keep_prob) )\n","\n","\t\tself.output_layer = nn.Linear(hidden_size, vocab_size)\n","\n","\t\tself.init_weights()\n","\n","\n","\tdef init_weights(self):\n","\n","\t\tnn.init.uniform_(self.embedding_layer.weight, -0.1, 0.1)\n","\n","\t\tfor hidden_layer in self.hidden_layers:\n","\t\t\thidden_layer.init_weights()\n","\n","\t\tnn.init.uniform_(self.output_layer.weight, -0.1, 0.1)\n","\t\tnn.init.constant_(self.output_layer.bias, 0)\n","\n","\tdef init_hidden(self):\n","  \t\n","\t\treturn torch.zeros((self.num_layers, self.batch_size, self.hidden_size))\n","    \n","\tdef forward(self, x, h_prev_t):\n","\n","\t\tx = self.embedding_layer(x)\n","\n","\t\tlogits = torch.zeros((self.seq_len, self.batch_size, self.vocab_size), device=x.device)\n","\n","\t\tfor t in range(self.seq_len):\n","\t\t\t\n","\t\t\th_outs = []\n","\n","\t\t\th_prev_l = x[t]\n","\n","\t\t\tfor l, h_l in enumerate(self.hidden_layers):\n","\n","\t\t\t\th_l_out = h_l(h_prev_l, h_prev_t[l])\n","\n","\t\t\t\th_outs.append(h_l_out)\n","\n","\t\t\t\th_prev_l = h_l_out\n","\n","\t\t\th_prev_t = torch.stack(h_outs)\n","\n","\t\t\th_prev_l = self.dropout(h_prev_l)\n","\n","\t\t\tlogits[t] = self.output_layer(h_prev_l)\n","\n","\t\treturn logits.view(self.seq_len, self.batch_size, self.vocab_size), h_prev_t\n","\n","\n","\tdef generate(self, x, h_prev_t, generated_seq_len):\n","\n","\t\tsamples = x.view(1, -1)\n","\n","\t\tx = self.embedding_layer(samples)\n","\t\n","\t\tfor _ in range(generated_seq_len):\n","\t\t\t\n","\t\t\th_outs = []\n","\n","\t\t\th_prev_l = x[0]\n","\n","\t\t\tfor l, h_l in enumerate(self.hidden_layers):\n","\n","\t\t\t\th_l_out = h_l(h_prev_l, h_prev_t[l])\n","\n","\t\t\t\th_outs.append(h_l_out)\n","\n","\t\t\t\th_prev_l = h_l_out\n","\n","\t\t\th_prev_t = torch.stack(h_outs)\n","\n","\t\t\th_prev_l = self.dropout(h_prev_l)\n","\n","\t\t\tlogits = self.output_layer(h_prev_l)\n","\n","\t\t\ttoken = torch.argmax( nn.Softmax(logits), dim=1 ).detach().view(1, -1)\n","\n","\t\t\tsamples = torch.cat( (samples, token), dim=0 )\n","\n","\t\t\tx = self.embedding_layer(token)\n","\n","\t\treturn samples"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PN0KdeIN6ipB","colab_type":"text"},"cell_type":"markdown","source":["# GRU"]},{"metadata":{"id":"vElw6SWO5H6p","colab_type":"code","colab":{}},"cell_type":"code","source":["class GRUCell(nn.Module):\n","  \n","\tdef __init__(self, input_size, hidden_size, dp_keep_prob):\n","    \n","\t\tsuper(GRUCell, self).__init__()\n","\n","\t\tself.hidden_size = hidden_size\n","\n","\t\tself.fc_r = nn.Linear(input_size + hidden_size, hidden_size)\n","\t\tself.fc_z = nn.Linear(input_size + hidden_size, hidden_size)\n","\t\tself.fc_h = nn.Linear(input_size + hidden_size, hidden_size)\n","\n","\t\tself.dropout = nn.Dropout(1 - dp_keep_prob)\n","\t\tself.sigmoid = nn.Sigmoid()\n","\t\tself.tanh = nn.Tanh()\n","\n","\n","\tdef init_weights(self):\n","\n","\t\tk = np.sqrt(1 / self.hidden_size)\n","\n","\t\tnn.init.uniform_(self.fc_r.weight, -k, k)\n","\t\tnn.init.uniform_(self.fc_r.bias, -k, k)\n","\n","\t\tnn.init.uniform_(self.fc_z.weight, -k, k)\n","\t\tnn.init.uniform_(self.fc_z.bias, -k, k)\n","\n","\t\tnn.init.uniform_(self.fc_h.weight, -k, k)\n","\t\tnn.init.uniform_(self.fc_h.bias, -k, k)\n","\t\t\n","\n","\tdef forward(self, x, h):\n","\t\n","\t\tx = self.dropout(x)\n","\n","\t\tx__h = torch.cat((x, h), dim=1)\n","\n","\t\tr = self.sigmoid( self.fc_r(x__h) )\n","\t\tz = self.sigmoid( self.fc_z(x__h) )\n","\n","\t\tx__r__h = torch.cat((x, r*h), dim=1)\n","\n","\t\th_tilde = self.tanh( self.fc_h(x__r__h) )\n","\t\th = (1-z) * h + z * h_tilde\n","\n","\t\treturn h\t\n","\n","class GRU(nn.Module):\n","\n","\tdef __init__(self, emb_size, hidden_size, seq_len, batch_size, vocab_size, num_layers, dp_keep_prob):\n","\n","\t\tsuper(GRU, self).__init__()\n","\n","\t\tself.emb_size = emb_size\n","\t\tself.hidden_size = hidden_size\n","\t\tself.seq_len = seq_len\n","\t\tself.batch_size = batch_size\n","\t\tself.vocab_size = vocab_size\n","\t\tself.num_layers = num_layers\n","\t\tself.dp_keep_prob = dp_keep_prob\n","\n","\t\tself.dropout = nn.Dropout(1 - dp_keep_prob)\n","\n","\t\tself.embedding_layer = nn.Embedding(vocab_size, emb_size)\n","\n","\t\tself.hidden_layers = nn.ModuleList()\n","\n","\t\tfor i in range(num_layers):\n","\n","\t\t\tif i == 0:\n","\t\t\t\tself.hidden_layers.append( GRUCell(emb_size, hidden_size, dp_keep_prob) )\n","\t\t\telse:\n","\t\t\t\tself.hidden_layers.append( GRUCell(hidden_size, hidden_size, dp_keep_prob) )\n","\n","\t\tself.output_layer = nn.Linear(hidden_size, vocab_size)\n","\n","\t\tself.init_weights()\n","\n","\tdef init_weights(self):\n","    \n","\t\tnn.init.uniform_(self.embedding_layer.weight, -0.1, 0.1)\n","\n","\t\tfor hidden_layer in self.hidden_layers:\n","\t\t\thidden_layer.init_weights()\n","\n","\t\tnn.init.uniform_(self.output_layer.weight, -0.1, 0.1)\n","\t\tnn.init.constant_(self.output_layer.bias, 0)\n","\n","\tdef init_hidden(self):\n","    \n","\t\treturn torch.zeros((self.num_layers, self.batch_size, self.hidden_size))\n","\n","\tdef forward(self, x, h_prev_t):\n","\n","\t\tx = self.embedding_layer(x)\n","\n","\t\tlogits = torch.zeros((self.seq_len, self.batch_size, self.vocab_size), device=x.device)\n","\n","\t\tfor t in range(self.seq_len):\n","\t\t\t\n","\t\t\th_outs = []\n","\n","\t\t\th_prev_l = x[t]\n","\n","\t\t\tfor l, h_l in enumerate(self.hidden_layers):\n","\n","\t\t\t\th_l_out = h_l(h_prev_l, h_prev_t[l])\n","\n","\t\t\t\th_outs.append(h_l_out)\n","\n","\t\t\t\th_prev_l = h_l_out\n","\n","\t\t\th_prev_t = torch.stack(h_outs)\n","\n","\t\t\th_prev_l = self.dropout(h_prev_l)\n","\n","\t\t\tlogits[t] = self.output_layer(h_prev_l)\n","\n","\t\treturn logits.view(self.seq_len, self.batch_size, self.vocab_size), h_prev_t\n","\n","\tdef generate(self, x, h_prev_t, generated_seq_len):\n","\n","\t\tsamples = x.view(1, -1)\n","\n","\t\tx = self.embedding_layer(samples)\n","\t\n","\t\tfor _ in range(generated_seq_len):\n","\t\t\t\n","\t\t\th_outs = []\n","\n","\t\t\th_prev_l = x[0]\n","\n","\t\t\tfor l, h_l in enumerate(self.hidden_layers):\n","\n","\t\t\t\th_l_out = h_l(h_prev_l, h_prev_t[l])\n","\n","\t\t\t\th_outs.append(h_l_out)\n","\n","\t\t\t\th_prev_l = h_l_out\n","\n","\t\t\th_prev_t = torch.stack(h_outs)\n","\n","\t\t\th_prev_l = self.dropout(h_prev_l)\n","\n","\t\t\tlogits = self.output_layer(h_prev_l)\n","\n","\t\t\ttoken = torch.argmax( nn.Softmax(logits), dim=1 ).detach().view(1, -1)\n","\n","\t\t\tsamples = torch.cat( (samples, token), dim=0 )\n","\n","\t\t\tx = self.embedding_layer(token)\n","\n","\t\treturn samples"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Nu6i-dstiJKm","colab_type":"text"},"cell_type":"markdown","source":["# Transformer"]},{"metadata":{"id":"pIpAAzkUiMXd","colab_type":"code","colab":{}},"cell_type":"code","source":["class MultiHeadedAttention(nn.Module):\n","    def __init__(self, n_heads, n_units, dropout=0.1):\n","        \"\"\"\n","        n_heads: the number of attention heads\n","        n_units: the number of input and output units\n","        dropout: probability of DROPPING units\n","        \"\"\"\n","        super(MultiHeadedAttention, self).__init__()\n","        # This sets the size of the keys, values, and queries (self.d_k) to all\n","        # be equal to the number of output units divided by the number of heads.\n","        self.d_k = n_units // n_heads\n","        # This requires the number of n_heads to evenly divide n_units.\n","        assert n_units % n_heads == 0\n","        self.n_units = n_units\n","\n","        # TODO: create/initialize any necessary parameters or layers\n","        # Initialize all weights and biases uniformly in the range [-k, k],\n","        # where k is the square root of 1/n_units.\n","        # Note: the only Pytorch modules you are allowed to use are nn.Linear\n","        # and nn.Dropout\n","        # ETA: you can also use softmax\n","        # ETA: you can use the \"clones\" function we provide.\n","\n","        # ---- MY CODE STARTS HERE ----\n","        self.n_heads = n_heads\n","        self.attn_transform = clones(nn.Linear(self.n_units, self.n_units), 4)\n","        # self.outp_transform = nn.Linear(self.n_units, self.n_units)\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        for nm, param in self.named_parameters():\n","            if param.requires_grad:\n","                k = 1.0 / math.sqrt(param.size(0))\n","                nn.init.uniform_(param, -k, k)\n","                print(nm, '\\t', param.size())\n","\n","    def forward(self, query, key, value, mask=None):\n","        # TODO: implement the masked multi-head attention.\n","        # query, key, and value correspond to Q, K, and V in the latex, and\n","        # they all have size: (batch_size, seq_len, self.n_units)\n","        # mask has size: (batch_size, seq_len, seq_len)\n","        # As described in the .tex, apply input masking to the softmax\n","        # generating the \"attention values\" (i.e. A_i in the .tex)\n","        # Also apply dropout to the attention values.\n","\n","        # ---- MY CODE STARTS HERE ----\n","\n","        batch_size = query.shape[0]\n","\n","        query, key, value = [linear(x).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2) \\\n","                             for x, linear in zip((query, key, value), self.attn_transform)]  # batch_size, n_heads, seq_len, self.d_k\n","\n","        # scaled self attention\n","        # batch_size, n_heads, seq_len, seq_len\n","        score = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k)\n","\n","        if mask is not None:\n","            # batch_size, 1, seq_len, seq_len\n","            mask = mask.unsqueeze(1)\n","            score = score.masked_fill(mask == 0, -1e9)\n","\n","        score = F.softmax(score, dim=-1)\n","        score = self.dropout(score)\n","\n","        # batch_size, self.n_heads, seq_len, self.d_k\n","        weighted_value = torch.matmul(score, value)\n","        # batch_size, self.n_heads, seq_len, self.n_units\n","        weighted_value = weighted_value.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_k)\n","\n","        # return self.outp_transform(weighted_value)\n","        return self.attn_transform[-1](weighted_value)\n","\n","\n","# ----------------------------------------------------------------------------------\n","# The encodings of elements of the input sequence\n","\n","class WordEmbedding(nn.Module):\n","    def __init__(self, n_units, vocab):\n","        super(WordEmbedding, self).__init__()\n","        self.lut = nn.Embedding(vocab, n_units)\n","        self.n_units = n_units\n","\n","    def forward(self, x):\n","        # print (x)\n","        return self.lut(x) * math.sqrt(self.n_units)\n","\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, n_units, dropout, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        # Compute the positional encodings once in log space.\n","        pe = torch.zeros(max_len, n_units)\n","        position = torch.arange(0, max_len).unsqueeze(1).float()\n","        div_term = torch.exp(torch.arange(0, n_units, 2).float() *\n","                             -(math.log(10000.0) / n_units))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + Variable(self.pe[:, :x.size(1)],\n","                         requires_grad=False)\n","        return self.dropout(x)\n","\n","\n","# ----------------------------------------------------------------------------------\n","# The TransformerBlock and the full Transformer\n","\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, size, self_attn, feed_forward, dropout):\n","        super(TransformerBlock, self).__init__()\n","        self.size = size\n","        self.self_attn = self_attn\n","        self.feed_forward = feed_forward\n","        self.sublayer = clones(ResidualSkipConnectionWithLayerNorm(size, dropout), 2)\n","\n","    def forward(self, x, mask):\n","        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))  # apply the self-attention\n","        return self.sublayer[1](x, self.feed_forward)  # apply the position-wise MLP\n","\n","\n","class TransformerStack(nn.Module):\n","    \"\"\"\n","    This will be called on the TransformerBlock (above) to create a stack.\n","    \"\"\"\n","\n","    def __init__(self, layer, n_blocks):  # layer will be TransformerBlock (below)\n","        super(TransformerStack, self).__init__()\n","        self.layers = clones(layer, n_blocks)\n","        self.norm = LayerNorm(layer.size)\n","\n","    def forward(self, x, mask):\n","        for layer in self.layers:\n","            x = layer(x, mask)\n","        return self.norm(x)\n","\n","\n","class FullTransformer(nn.Module):\n","    def __init__(self, transformer_stack, embedding, n_units, vocab_size):\n","        super(FullTransformer, self).__init__()\n","        self.transformer_stack = transformer_stack\n","        self.embedding = embedding\n","        self.output_layer = nn.Linear(n_units, vocab_size)\n","\n","    def forward(self, input_sequence, mask):\n","        embeddings = self.embedding(input_sequence)\n","        return F.log_softmax(self.output_layer(self.transformer_stack(embeddings, mask)), dim=-1)\n","\n","\n","def make_model(vocab_size, n_blocks=6,\n","               n_units=512, n_heads=16, dropout=0.1):\n","    \"Helper: Construct a model from hyperparameters.\"\n","    c = copy.deepcopy\n","    attn = MultiHeadedAttention(n_heads, n_units)\n","    ff = MLP(n_units, dropout)\n","    position = PositionalEncoding(n_units, dropout)\n","    model = FullTransformer(\n","        transformer_stack=TransformerStack(TransformerBlock(n_units, c(attn), c(ff), dropout), n_blocks),\n","        embedding=nn.Sequential(WordEmbedding(n_units, vocab_size), c(position)),\n","        n_units=n_units,\n","        vocab_size=vocab_size\n","    )\n","\n","    # Initialize parameters with Glorot / fan_avg.\n","    for p in model.parameters():\n","        if p.dim() > 1:\n","            nn.init.xavier_uniform_(p)\n","    return model\n","\n","\n","# ----------------------------------------------------------------------------------\n","# Data processing\n","\n","def subsequent_mask(size):\n","    \"\"\" helper function for creating the masks. \"\"\"\n","    attn_shape = (1, size, size)\n","    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n","    return torch.from_numpy(subsequent_mask) == 0\n","\n","\n","class Batch:\n","    \"Object for holding a batch of data with mask during training.\"\n","\n","    def __init__(self, x, pad=0):\n","        self.data = x\n","        self.mask = self.make_mask(self.data, pad)\n","\n","    @staticmethod\n","    def make_mask(data, pad):\n","        \"Create a mask to hide future words.\"\n","        mask = (data != pad).unsqueeze(-2)\n","        mask = mask & Variable(\n","            subsequent_mask(data.size(-1)).type_as(mask.data))\n","        return mask\n","\n","\n","#----------------------------------------------------------------------------------\n","# Some standard modules\n","\n","class LayerNorm(nn.Module):\n","    \"layer normalization, as in: https://arxiv.org/abs/1607.06450\"\n","    def __init__(self, features, eps=1e-6):\n","        super(LayerNorm, self).__init__()\n","        self.a_2 = nn.Parameter(torch.ones(features))\n","        self.b_2 = nn.Parameter(torch.zeros(features))\n","        self.eps = eps\n","\n","    def forward(self, x):\n","        mean = x.mean(-1, keepdim=True)\n","        std = x.std(-1, keepdim=True)\n","        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n","\n","\n","class ResidualSkipConnectionWithLayerNorm(nn.Module):\n","    \"\"\"\n","    A residual connection followed by a layer norm.\n","    Note for code simplicity the norm is first as opposed to last.\n","    \"\"\"\n","    def __init__(self, size, dropout):\n","        super(ResidualSkipConnectionWithLayerNorm, self).__init__()\n","        self.norm = LayerNorm(size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, sublayer):\n","        \"Apply residual connection to any sublayer with the same size.\"\n","        return x + self.dropout(sublayer(self.norm(x)))\n","\n","\n","class MLP(nn.Module):\n","    \"\"\"\n","    This is just an MLP with 1 hidden layer\n","    \"\"\"\n","    def __init__(self, n_units, dropout=0.1):\n","        super(MLP, self).__init__()\n","        self.w_1 = nn.Linear(n_units, 2048)\n","        self.w_2 = nn.Linear(2048, n_units)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        return self.w_2(self.dropout(F.relu(self.w_1(x))))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sFIMUkyM6mNo","colab_type":"text"},"cell_type":"markdown","source":["# Setup"]},{"metadata":{"id":"HZNJsVMsD4um","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"5dfec90e-02bf-4dd2-b734-63a607bf4117","executionInfo":{"status":"ok","timestamp":1552930335228,"user_tz":240,"elapsed":2268,"user":{"displayName":"Maximilien Le Clei","photoUrl":"https://lh4.googleusercontent.com/-MWdkEHlJfJ8/AAAAAAAAAAI/AAAAAAAAALk/iuE1yhmpMCI/s64/photo.jpg","userId":"17481473454263177289"}}},"cell_type":"code","source":["##############################################################################\n","#\n","# ARG PARSING AND EXPERIMENT SETUP\n","#\n","##############################################################################\n","\n","parser = argparse.ArgumentParser(description='PyTorch Penn Treebank Language Modeling')\n","\n","# Arguments you may need to set to run different experiments in 4.1 & 4.2.\n","parser.add_argument('--data', type=str, default= path + 'data',\n","                    help='location of the data corpus. We suggest you change the default\\\n","                    here, rather than passing as an argument, to avoid long file paths.')\n","parser.add_argument('--model', type=str, default='RNN',\n","                    help='type of recurrent net (RNN, GRU, TRANSFORMER)')\n","parser.add_argument('--optimizer', type=str, default='ADAM',\n","                    help='optimization algo to use; SGD, SGD_LR_SCHEDULE, ADAM')\n","parser.add_argument('--seq_len', type=int, default=35,\n","                    help='number of timesteps over which BPTT is performed')\n","parser.add_argument('--batch_size', type=int, default=20,\n","                    help='size of one minibatch')\n","parser.add_argument('--initial_lr', type=float, default=0.0001,\n","                    help='initial learning rate')\n","parser.add_argument('--hidden_size', type=int, default=1500,\n","                    help='size of hidden layers. IMPORTANT: for the transformer\\\n","                    this must be a multiple of 16.')\n","#parser.add_argument('--save_best', action='store_true',\n","                    #help='save the model for the best validation performance')\n","parser.add_argument('--num_layers', type=int, default=2,\n","                    help='number of hidden layers in RNN/GRU, or number of transformer blocks in TRANSFORMER')\n","\n","# Other hyperparameters you may want to tune in your exploration\n","parser.add_argument('--emb_size', type=int, default=200,\n","                    help='size of word embeddings')\n","parser.add_argument('--num_epochs', type=int, default=40,\n","                    help='number of epochs to stop after')\n","parser.add_argument('--dp_keep_prob', type=float, default=0.35,\n","                    help='dropout *keep* probability. drop_prob = 1-dp_keep_prob \\\n","                    (dp_keep_prob=1 means no dropout)')\n","\n","# Arguments that you may want to make use of / implement more code for\n","parser.add_argument('--debug', action='store_true') \n","parser.add_argument('--save_dir', type=str, default=path,\n","                    help='path to save the experimental config, logs, model \\\n","                    This is automatically generated based on the command line \\\n","                    arguments you pass and only needs to be set if you want a \\\n","                    custom dir name')\n","parser.add_argument('--evaluate', action='store_true',\n","                    help=\"use this flag to run on the test set. Only do this \\\n","                    ONCE for each model setting, and only after you've \\\n","                    completed ALL hyperparameter tuning on the validation set.\\\n","                    Note we are not requiring you to do this.\")\n","\n","# DO NOT CHANGE THIS (setting the random seed makes experiments deterministic, \n","# which helps for reproducibility)\n","parser.add_argument('--seed', type=int, default=1111,\n","                    help='random seed')\n","\n","args = parser.parse_args(args=[])\n","argsdict = args.__dict__\n","argsdict['code_file'] = sys.argv[0]\n","\n","# Use the model, optimizer, and the flags passed to the script to make the \n","# name for the experimental dir\n","print(\"\\n########## Setting Up Experiment ######################\")\n","flags = [flag.lstrip('--') for flag in sys.argv[1:]]\n","experiment_path = os.path.join(args.save_dir+'_'.join([argsdict['model'],\n","                                         argsdict['optimizer']] \n","                                         ))\n","\n","# Increment a counter so that previous results with the same args will not\n","# be overwritten. Comment out the next four lines if you only want to keep\n","# the most recent results.\n","i = 0\n","while os.path.exists(experiment_path + \"_\" + str(i)):\n","    i += 1\n","experiment_path = experiment_path + \"_\" + str(i)\n","\n","# Creates an experimental directory and dumps all the args to a text file\n","os.mkdir(experiment_path)\n","print (\"\\nPutting log in %s\"%experiment_path)\n","argsdict['save_dir'] = experiment_path\n","with open (os.path.join(experiment_path,'exp_config.txt'), 'w') as f:\n","    for key in sorted(argsdict):\n","        f.write(key+'    '+str(argsdict[key])+'\\n')\n","\n","# Set the random seed manually for reproducibility.\n","torch.manual_seed(args.seed)\n","\n","# Use the GPU if you have one\n","if torch.cuda.is_available():\n","    print(\"Using the GPU\")\n","    device = torch.device(\"cuda\") \n","else:\n","    print(\"WARNING: You are about to run on cpu, and this will likely run out \\\n","      of memory. \\n You can try setting batch_size=1 to reduce memory usage\")\n","    device = torch.device(\"cpu\")\n","\n","\n","###############################################################################\n","#\n","# DATA LOADING & PROCESSING\n","#\n","###############################################################################\n","\n","# HELPER FUNCTIONS\n","def _read_words(filename):\n","    with open(filename, \"r\") as f:\n","      return f.read().replace(\"\\n\", \"<eos>\").split()\n","\n","def _build_vocab(filename):\n","    data = _read_words(filename)\n","\n","    counter = collections.Counter(data)\n","    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n","\n","    words, _ = list(zip(*count_pairs))\n","    word_to_id = dict(zip(words, range(len(words))))\n","    id_to_word = dict((v, k) for k, v in word_to_id.items())\n","\n","    return word_to_id, id_to_word\n","\n","def _file_to_word_ids(filename, word_to_id):\n","    data = _read_words(filename)\n","    return [word_to_id[word] for word in data if word in word_to_id]\n","\n","# Processes the raw data from text files\n","def ptb_raw_data(data_path=None, prefix=\"ptb\"):\n","    train_path = os.path.join(data_path, prefix + \".train.txt\")\n","    valid_path = os.path.join(data_path, prefix + \".valid.txt\")\n","    test_path = os.path.join(data_path, prefix + \".test.txt\")\n","\n","    word_to_id, id_2_word = _build_vocab(train_path)\n","    train_data = _file_to_word_ids(train_path, word_to_id)\n","    valid_data = _file_to_word_ids(valid_path, word_to_id)\n","    test_data = _file_to_word_ids(test_path, word_to_id)\n","    return train_data, valid_data, test_data, word_to_id, id_2_word\n","\n","# Yields minibatches of data\n","def ptb_iterator(raw_data, batch_size, num_steps):\n","    raw_data = np.array(raw_data, dtype=np.int32)\n","\n","    data_len = len(raw_data)\n","    batch_len = data_len // batch_size\n","    data = np.zeros([batch_size, batch_len], dtype=np.int32)\n","    for i in range(batch_size):\n","        data[i] = raw_data[batch_len * i:batch_len * (i + 1)]\n","\n","    epoch_size = (batch_len - 1) // num_steps\n","\n","    if epoch_size == 0:\n","        raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n","\n","    for i in range(epoch_size):\n","        x = data[:, i*num_steps:(i+1)*num_steps]\n","        y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n","        yield (x, y)\n","\n","\n","class Batch:\n","    \"Data processing for the transformer. This class adds a mask to the data.\"\n","    def __init__(self, x, pad=-1):\n","        self.data = x\n","        self.mask = self.make_mask(self.data, pad)\n","    \n","    @staticmethod\n","    def make_mask(data, pad):\n","        \"Create a mask to hide future words.\"\n","\n","        def subsequent_mask(size):\n","            \"\"\" helper function for creating the masks. \"\"\"\n","            attn_shape = (1, size, size)\n","            subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n","            return torch.from_numpy(subsequent_mask) == 0\n","\n","        mask = (data != pad).unsqueeze(-2)\n","        mask = mask & Variable(\n","            subsequent_mask(data.size(-1)).type_as(mask.data))\n","        return mask\n","\n","\n","# LOAD DATA\n","print('Loading data from '+args.data)\n","raw_data = ptb_raw_data(data_path=args.data)\n","train_data, valid_data, test_data, word_to_id, id_2_word = raw_data\n","vocab_size = len(word_to_id)\n","print('  vocabulary size: {}'.format(vocab_size))\n","\n","\n","###############################################################################\n","# \n","# MODEL SETUP\n","#\n","###############################################################################\n","\n","# NOTE ==============================================\n","# This is where your model code will be called. You may modify this code\n","# if required for your implementation, but it should not typically be necessary,\n","# and you must let the TAs know if you do so.\n","if args.model == 'RNN':\n","    model = RNN(emb_size=args.emb_size, hidden_size=args.hidden_size, \n","                seq_len=args.seq_len, batch_size=args.batch_size,\n","                vocab_size=vocab_size, num_layers=args.num_layers, \n","                dp_keep_prob=args.dp_keep_prob)\n","elif args.model == 'GRU':\n","    model = GRU(emb_size=args.emb_size, hidden_size=args.hidden_size,\n","                seq_len=args.seq_len, batch_size=args.batch_size,\n","                vocab_size=vocab_size, num_layers=args.num_layers,\n","                dp_keep_prob=args.dp_keep_prob)\n","elif args.model == 'TRANSFORMER':\n","    if args.debug:  # use a very small model\n","        model = TRANSFORMER(vocab_size=vocab_size, n_units=16, n_blocks=2)\n","    else:\n","        # Note that we're using num_layers and hidden_size to mean slightly\n","        # different things here than in the RNNs.\n","        # Also, the Transformer also has other hyperparameters\n","        # (such as the number of attention heads) which can change it's behavior.\n","        model = TRANSFORMER(vocab_size=vocab_size, n_units=args.hidden_size,\n","                            n_blocks=args.num_layers, dropout=1.-args.dp_keep_prob)\n","    # these 3 attributes don't affect the Transformer's computations;\n","    # they are only used in run_epoch\n","    model.batch_size = args.batch_size\n","    model.seq_len = args.seq_len\n","    model.vocab_size = vocab_size\n","else:\n","    print(\"Model type not recognized.\")\n","\n","\n","model = model.to(device)\n","\n","# LOSS FUNCTION\n","loss_fn = nn.CrossEntropyLoss()\n","if args.optimizer == 'ADAM':\n","    optimizer = torch.optim.Adam(model.parameters(), lr=args.initial_lr)\n","\n","# LEARNING RATE SCHEDULE    \n","lr = args.initial_lr\n","lr_decay_base = 1 / 1.15\n","m_flat_lr = 14.0 # we will not touch lr for the first m_flat_lr epochs\n","\n","\n","###############################################################################\n","# \n","# DEFINE COMPUTATIONS FOR PROCESSING ONE EPOCH\n","#\n","###############################################################################\n","\n","def repackage_hidden(h):\n","    \"\"\"\n","    Wraps hidden states in new Tensors, to detach them from their history.\n","    \n","    This prevents Pytorch from trying to backpropagate into previous input \n","    sequences when we use the final hidden states from one mini-batch as the \n","    initial hidden states for the next mini-batch.\n","    \n","    Using the final hidden states in this way makes sense when the elements of \n","    the mini-batches are actually successive subsequences in a set of longer sequences.\n","    This is the case with the way we've processed the Penn Treebank dataset.\n","    \"\"\"\n","    if isinstance(h, Variable):\n","        return h.detach_()\n","    else:\n","        return tuple(repackage_hidden(v) for v in h)\n","\n","\n","def run_epoch(model, data, is_train=False, lr=1.0):\n","    \"\"\"\n","    One epoch of training/validation (depending on flag is_train).\n","    \"\"\"\n","    if is_train:\n","        model.train()\n","    else:\n","        model.eval()\n","    epoch_size = ((len(data) // model.batch_size) - 1) // model.seq_len\n","    start_time = time.time()\n","    if args.model != 'TRANSFORMER':\n","        hidden = model.init_hidden()\n","        hidden = hidden.to(device)\n","    costs = 0.0\n","    iters = 0\n","    b_time, f_time = 0, 0\n","    losses = []\n","\n","    # LOOP THROUGH MINIBATCHES\n","    for step, (x, y) in enumerate(ptb_iterator(data, model.batch_size, model.seq_len)):\n","        if args.model == 'TRANSFORMER':\n","            batch = Batch(torch.from_numpy(x).long().to(device))\n","            model.zero_grad()\n","            forward_time = time.time()\n","            outputs = model.forward(batch.data, batch.mask).transpose(1,0)\n","            f_time += time.time() - forward_time\n","            #print (\"outputs.shape\", outputs.shape)\n","        else:\n","            inputs = torch.from_numpy(x.astype(np.int64)).transpose(0, 1).contiguous().to(device)#.cuda()\n","            model.zero_grad()\n","            hidden = repackage_hidden(hidden)\n","            \n","            forward_time = time.time()\n","            outputs, hidden = model(inputs, hidden)\n","            f_time += time.time() - forward_time\n","\n","        targets = torch.from_numpy(y.astype(np.int64)).transpose(0, 1).contiguous().to(device)#.cuda()\n","        tt = torch.squeeze(targets.view(-1, model.batch_size * model.seq_len))\n","\n","        # LOSS COMPUTATION\n","        # This line currently averages across all the sequences in a mini-batch \n","        # and all time-steps of the sequences.\n","        # For problem 5.3, you will (instead) need to compute the average loss \n","        #at each time-step separately. \n","        loss = loss_fn(outputs.contiguous().view(-1, model.vocab_size), tt)\n","        costs += loss.data.item() * model.seq_len\n","        losses.append(costs)\n","        iters += model.seq_len\n","        if args.debug:\n","            print(step, loss)\n","        if is_train:  # Only update parameters if training \n","            backward_time = time.time()\n","            loss.backward()\n","            b_time += time.time() - backward_time\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n","            if args.optimizer == 'ADAM':\n","                optimizer.step()\n","            else: \n","                for p in model.parameters():\n","                    if p.grad is not None:\n","                        p.data.add_(-lr, p.grad.data)\n","            if step % 10 == 0:\n","              print('\\rstep: {}; loss: {:.5f}; costs: {:.2f}; speed (wps) {:.2f}; b_time = {:.2f}; f_time = {:.2f}'\n","                    ''.format(step, loss, costs, iters * model.batch_size / (time.time() - start_time), b_time, f_time),\n","                    end='')\n","    print('')          \n","    return np.exp(costs / iters), losses\n"],"execution_count":22,"outputs":[{"output_type":"stream","text":["\n","########## Setting Up Experiment ######################\n","\n","Putting log in drive/My Drive/IFT6135/A2/RNN_ADAM_2\n","Using the GPU\n","Loading data from drive/My Drive/IFT6135/A2/data\n","  vocabulary size: 10000\n"],"name":"stdout"}]},{"metadata":{"id":"A531FyM34Tzv","colab_type":"text"},"cell_type":"markdown","source":["# Run"]},{"metadata":{"id":"gSzzTmARHhuD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1037},"outputId":"b74b9c5d-6c74-4a30-f17d-ea4eff25936d"},"cell_type":"code","source":["###############################################################################\n","#\n","# RUN MAIN LOOP (TRAIN AND VAL)\n","#\n","###############################################################################\n","\n","print(\"\\n########## Running Main Loop ##########################\")\n","train_ppls = []\n","train_losses = []\n","val_ppls = []\n","val_losses = []\n","best_val_so_far = np.inf\n","times = []\n","\n","# In debug mode, only run one epoch\n","if args.debug:\n","    num_epochs = 1 \n","else:\n","    num_epochs = args.num_epochs\n","\n","# MAIN LOOP\n","for epoch in range(num_epochs):\n","    t0 = time.time()\n","    print('\\nEPOCH '+str(epoch)+' ------------------')\n","    if args.optimizer == 'SGD_LR_SCHEDULE':\n","        lr_decay = lr_decay_base ** max(epoch - m_flat_lr, 0)\n","        lr = lr * lr_decay # decay lr if it is time\n","\n","    # RUN MODEL ON TRAINING DATA\n","    train_ppl, train_loss = run_epoch(model, train_data, True, lr)\n","\n","    # RUN MODEL ON VALIDATION DATA\n","    val_ppl, val_loss = run_epoch(model, valid_data)\n","\n","\n","    # SAVE MODEL IF IT'S THE BEST SO FAR\n","    if val_ppl < best_val_so_far:\n","        best_val_so_far = val_ppl\n","        #if args.save_best:\n","            #print(\"Saving model parameters to best_params.pt\")\n","            #torch.save(model.state_dict(), os.path.join(args.save_dir, 'best_params.pt'))\n","        # NOTE ==============================================\n","        # You will need to load these parameters into the same model\n","        # for a couple Problems: so that you can compute the gradient \n","        # of the loss w.r.t. hidden state as required in Problem 5.2\n","        # and to sample from the the model as required in Problem 5.3\n","        # We are not asking you to run on the test data, but if you \n","        # want to look at test performance you would load the saved\n","        # model and run on the test data with batch_size=1\n","\n","    # LOC RESULTS\n","    train_ppls.append(train_ppl)\n","    val_ppls.append(val_ppl)\n","    train_losses.extend(train_loss)\n","    val_losses.extend(val_loss)\n","    times.append(time.time() - t0)\n","    log_str = 'epoch: ' + str(epoch) + '\\t' \\\n","            + 'train ppl: ' + str(train_ppl) + '\\t' \\\n","            + 'val ppl: ' + str(val_ppl)  + '\\t' \\\n","            + 'best val: ' + str(best_val_so_far) + '\\t' \\\n","            + 'time (s) spent in epoch: ' + str(times[-1])\n","    print(log_str)\n","    with open (os.path.join(args.save_dir, 'log.txt'), 'a') as f_:\n","        f_.write(log_str+ '\\n')\n","\n","# SAVE LEARNING CURVES\n","lc_path = os.path.join(args.save_dir, 'learning_curves.npy')\n","print('\\nDONE\\n\\nSaving learning curves to '+lc_path)\n","np.save(lc_path, {'train_ppls':train_ppls, \n","                  'val_ppls':val_ppls, \n","                  'train_losses':train_losses,\n","                  'val_losses':val_losses})\n","# NOTE ==============================================\n","# To load these, run \n","# >>> x = np.load(lc_path)[()]\n","# You will need these values for plotting learning curves (Problem 4)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","########## Running Main Loop ##########################\n","\n","EPOCH 0 ------------------\n","step: 1320; loss: 6.20033; costs: 300264.37; speed (wps) 2481.79; b_time = 75.26; f_time = 34.41\n","\n","epoch: 0\ttrain ppl: 660.6291308099272\tval ppl: 406.06040116658335\tbest val: 406.06040116658335\ttime (s) spent in epoch: 381.33693742752075\n","\n","EPOCH 1 ------------------\n","step: 1320; loss: 5.99346; costs: 278873.52; speed (wps) 2466.42; b_time = 75.40; f_time = 34.62\n","\n","epoch: 1\ttrain ppl: 416.4082569887265\tval ppl: 316.6891395129351\tbest val: 316.6891395129351\ttime (s) spent in epoch: 383.59803438186646\n","\n","EPOCH 2 ------------------\n","step: 1320; loss: 5.89427; costs: 270571.70; speed (wps) 2465.78; b_time = 75.42; f_time = 34.56\n","\n","epoch: 2\ttrain ppl: 348.01652517807895\tval ppl: 276.190021723623\tbest val: 276.190021723623\ttime (s) spent in epoch: 383.34928131103516\n","\n","EPOCH 3 ------------------\n","step: 1320; loss: 5.76911; costs: 265242.65; speed (wps) 2473.35; b_time = 75.29; f_time = 34.73\n","\n","epoch: 3\ttrain ppl: 310.1582766034095\tval ppl: 253.42798318218547\tbest val: 253.42798318218547\ttime (s) spent in epoch: 382.2621970176697\n","\n","EPOCH 4 ------------------\n","step: 1320; loss: 5.78391; costs: 261248.55; speed (wps) 2467.52; b_time = 75.39; f_time = 34.28\n","\n","epoch: 4\ttrain ppl: 284.5259387394909\tval ppl: 237.06723844146228\tbest val: 237.06723844146228\ttime (s) spent in epoch: 383.4095206260681\n","\n","EPOCH 5 ------------------\n","step: 1320; loss: 5.69631; costs: 258078.61; speed (wps) 2467.78; b_time = 75.41; f_time = 34.65\n","\n","epoch: 5\ttrain ppl: 265.6892870541976\tval ppl: 225.90659247197243\tbest val: 225.90659247197243\ttime (s) spent in epoch: 383.2762806415558\n","\n","EPOCH 6 ------------------\n","step: 1320; loss: 5.60064; costs: 255281.63; speed (wps) 2488.89; b_time = 75.21; f_time = 34.32\n","\n","epoch: 6\ttrain ppl: 250.10238826458226\tval ppl: 215.3737006283273\tbest val: 215.3737006283273\ttime (s) spent in epoch: 379.8461537361145\n","\n","EPOCH 7 ------------------\n","step: 1320; loss: 5.56673; costs: 252920.58; speed (wps) 2462.85; b_time = 75.49; f_time = 35.18\n","\n","epoch: 7\ttrain ppl: 237.65227281930393\tval ppl: 206.61476463648526\tbest val: 206.61476463648526\ttime (s) spent in epoch: 384.0582752227783\n","\n","EPOCH 8 ------------------\n","step: 1320; loss: 5.56922; costs: 250809.07; speed (wps) 2471.75; b_time = 75.40; f_time = 34.94\n","\n","epoch: 8\ttrain ppl: 227.04593085132078\tval ppl: 201.29694026464313\tbest val: 201.29694026464313\ttime (s) spent in epoch: 382.80777978897095\n","\n","EPOCH 9 ------------------\n","step: 1320; loss: 5.49451; costs: 248872.72; speed (wps) 2462.41; b_time = 75.51; f_time = 35.35\n","\n","epoch: 9\ttrain ppl: 217.72327062676453\tval ppl: 194.0502077242951\tbest val: 194.0502077242951\ttime (s) spent in epoch: 383.88045930862427\n","\n","EPOCH 10 ------------------\n","step: 1320; loss: 5.39786; costs: 247142.91; speed (wps) 2472.92; b_time = 75.40; f_time = 35.15\n","\n","epoch: 10\ttrain ppl: 209.7336226154672\tval ppl: 191.54204559511774\tbest val: 191.54204559511774\ttime (s) spent in epoch: 382.29060220718384\n","\n","EPOCH 11 ------------------\n","step: 90; loss: 5.21363; costs: 16838.37; speed (wps) 2500.76; b_time = 5.17; f_time = 2.42"],"name":"stdout"}]},{"metadata":{"id":"9JOp3JpvjS3_","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}