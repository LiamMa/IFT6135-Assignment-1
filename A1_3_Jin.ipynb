{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1_3_Jin.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "1eGIV7SLuBhP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Environment Setting and Data Preprocessing\n",
        "\n",
        "To reproduce the result, please put the dataset into a folder of google drive whose path is saved in *data_path*,  in our case, data_path = '/content/gdrive/My\\ Drive/Datasets/dogcat'"
      ]
    },
    {
      "metadata": {
        "id": "W9Scb4r5XYt3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 832
        },
        "outputId": "62a9dc50-498a-42de-c84e-e75a19aaee19"
      },
      "cell_type": "code",
      "source": [
        "! nvidia-smi\n",
        "! pip install mxnet-cu100"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Feb 11 05:13:46 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 410.79       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Collecting mxnet-cu100\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/d3/480e26ca6f0294ce268622834499f1efd82bebbf7ada9ec6ed4a67c6c890/mxnet_cu100-1.3.1-py2.py3-none-manylinux1_x86_64.whl (432.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 432.6MB 43kB/s \n",
            "\u001b[?25hCollecting requests>=2.20.0 (from mxnet-cu100)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e3/20f3d364d6c8e5d2353c72a67778eb189176f08e873c9900e10c0287b84b/requests-2.21.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 24.1MB/s \n",
            "\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1 (from mxnet-cu100)\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy<1.15.0,>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (1.14.6)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (2018.11.29)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (3.0.4)\n",
            "\u001b[31mspacy 2.0.18 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mgoogle-colab 0.0.1a1 has requirement requests~=2.18.0, but you'll have requests 2.21.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mcufflinks 0.14.6 has requirement plotly>=3.0.0, but you'll have plotly 1.12.12 which is incompatible.\u001b[0m\n",
            "Installing collected packages: requests, graphviz, mxnet-cu100\n",
            "  Found existing installation: requests 2.18.4\n",
            "    Uninstalling requests-2.18.4:\n",
            "      Successfully uninstalled requests-2.18.4\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-cu100-1.3.1 requests-2.21.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "z7yPuUVEwMGd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import mxnet as mx\n",
        "from mxnet import autograd, nd, init, gluon\n",
        "from mxnet.gluon import nn, loss as gloss, data as gdata\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import zipfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OQTNJuotEurB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We load the dataset and move files into the virtual machine environment of colab\n",
        " to speed up image loading."
      ]
    },
    {
      "metadata": {
        "id": "vk9K9HmXjiFm",
        "colab_type": "code",
        "outputId": "fd634d90-78e2-44b8-d6df-0c96dd31adf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "data_path = '/content/gdrive/My\\ Drive/Datasets/dogcat'\n",
        "base_path = '/content/dogcat'\n",
        "\n",
        "! rm -rf '/content/dogcat'\n",
        "! cp -r $data_path /content\n",
        "\n",
        "for f in ['trainset.zip', 'testset.zip']:\n",
        "    with zipfile.ZipFile(os.path.join(base_path, f)) as z:\n",
        "        z.extractall(base_path)\n",
        "        \n",
        "! rm -rf /content/dogcat/__MACOSX\n",
        "! ls /content/dogcat"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "sample_submission.csv  testset\ttestset.zip  trainset  trainset.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uVDMOat-E4p4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We divided the dataset into four folders:\n",
        "\n",
        "- trainset: the original training dataset which will be used to re-train the final model after parameter tuning.\n",
        "- testset: the original test dataset used to generate predictions.\n",
        "- train, valid: the training dataset and validation dataset used to train and select hyperparameters. They are splitted from trainset as a split ratio."
      ]
    },
    {
      "metadata": {
        "id": "H_4IhW_6w69l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "ba02c141-75dd-4ba0-8325-99a2b8fa6440"
      },
      "cell_type": "code",
      "source": [
        "train_valid_path = 'trainset'\n",
        "test_path = 'testset'\n",
        "train_path = 'train'\n",
        "valid_path = 'valid'\n",
        "\n",
        "\n",
        "def delete_dir(path):\n",
        "    if os.path.exists(os.path.join(*path)):\n",
        "        shutil.rmtree(os.path.join(*path))\n",
        "        \n",
        "def create_dir(path):\n",
        "    if not os.path.exists(os.path.join(*path)):\n",
        "        os.makedirs(os.path.join(*path))\n",
        "\n",
        "def devide_train_valid(base_path, train_valid_path, train_path, valid_path,\n",
        "                       valid_ratio=0.2, labels=['Dog', 'Cat']):\n",
        "    delete_dir([base_path, train_path])\n",
        "    delete_dir([base_path, valid_path])\n",
        "        \n",
        "    all_files_by_label = [os.listdir(os.path.join(base_path, train_valid_path, label))\n",
        "                          for label in labels]\n",
        "    \n",
        "    count_train = {label:0 for label in labels}\n",
        "    count_valid = {label:0 for label in labels}\n",
        "    \n",
        "    for label, file_per_label in zip(labels, all_files_by_label):\n",
        "        print('Train&Valid set, %s: %s' % (label, len(file_per_label)))\n",
        "        create_dir([base_path, train_path, label])\n",
        "        create_dir([base_path, valid_path, label])\n",
        "        for f in file_per_label:\n",
        "            if count_train[label] < len(file_per_label) * (1 - valid_ratio):\n",
        "                shutil.copy(os.path.join(base_path, train_valid_path, label, f),\n",
        "                            os.path.join(base_path, train_path, label, f))\n",
        "                count_train[label] += 1\n",
        "            else:\n",
        "                shutil.copy(os.path.join(base_path, train_valid_path, label, f),\n",
        "                            os.path.join(base_path, valid_path, label, f))\n",
        "                count_valid[label] += 1\n",
        "        print('Train set, %s: %s' % (label, count_train[label]))\n",
        "        print('Valid set, %s: %s' % (label, count_valid[label]))\n",
        "                \n",
        "devide_train_valid(base_path, train_valid_path, train_path, valid_path)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train&Valid set, Dog: 9999\n",
            "Train set, Dog: 8000\n",
            "Valid set, Dog: 1999\n",
            "Train&Valid set, Cat: 9999\n",
            "Train set, Cat: 8000\n",
            "Valid set, Cat: 1999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bF8LAVpLFscM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We defined four data iterators as below, given a batch size. We also have different data augmentation pipelines for train dataset and test dataset, since test dataset should keep unchanged."
      ]
    },
    {
      "metadata": {
        "id": "SsDH1T9pwG3B",
        "colab_type": "code",
        "outputId": "ace287cc-3a67-4eb2-c77a-2d964c770476",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "train_data = gdata.vision.ImageFolderDataset(\n",
        "    os.path.join(base_path, train_path), flag=1)\n",
        "valid_data = gdata.vision.ImageFolderDataset(\n",
        "    os.path.join(base_path, valid_path), flag=1)\n",
        "train_valid_data = gdata.vision.ImageFolderDataset(\n",
        "    os.path.join(base_path, train_valid_path), flag=1)\n",
        "test_data = gdata.vision.ImageFolderDataset(\n",
        "    os.path.join(base_path, test_path), flag=1)\n",
        "\n",
        "aug_train = gdata.vision.transforms.Compose([\n",
        "#     gdata.vision.transforms.RandomResizedCrop(64, scale=(0.75, 1),\n",
        "#                                                ratio=(3.0/4.0, 4.0/3.0)),\n",
        "    gdata.vision.transforms.RandomFlipLeftRight(),\n",
        "    gdata.vision.transforms.RandomColorJitter(brightness=0.4, \n",
        "                                              contrast=0.4, saturation=0.4),\n",
        "    gdata.vision.transforms.RandomLighting(0.1),\n",
        "    gdata.vision.transforms.ToTensor(),\n",
        "#     gdata.vision.transforms.Normalize([0.485, 0.456, 0.406], \n",
        "#                                       [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "aug_test = gdata.vision.transforms.Compose([\n",
        "    gdata.vision.transforms.ToTensor(),\n",
        "#     gdata.vision.transforms.Normalize([0.485, 0.456, 0.406], \n",
        "#                                       [0.229, 0.224, 0.225])\n",
        "])\n",
        "    \n",
        "def load_data_iter(batch_size):\n",
        "\n",
        "    train_iter = gdata.DataLoader(train_data.transform_first(aug_train), batch_size,\n",
        "                                  shuffle=True, last_batch='keep')\n",
        "    valid_iter = gdata.DataLoader(valid_data.transform_first(aug_test), batch_size,\n",
        "                                  shuffle=True, last_batch='keep')\n",
        "    train_valid_iter = gdata.DataLoader(train_valid_data.transform_first(aug_train), batch_size,\n",
        "                                  shuffle=True, last_batch='keep')\n",
        "    test_iter  = gdata.DataLoader(test_data.transform_first(aug_test), batch_size,\n",
        "                                  shuffle=False, last_batch='keep')\n",
        "    \n",
        "    return train_iter, valid_iter, train_valid_iter, test_iter"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Ignoring /content/dogcat/trainset/.DS_Store, which is not a directory.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Vk6fUe-Tn0lI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building VGG Model"
      ]
    },
    {
      "metadata": {
        "id": "GNviaRqLO7U2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# VGG modified\n",
        "class VGGBlock(nn.Block):\n",
        "    def __init__(self, num_conv, num_channel, **kwargs):\n",
        "        super(VGGBlock, self).__init__(**kwargs)\n",
        "        \n",
        "        self.net = nn.Sequential()\n",
        "        for i in range(num_conv):\n",
        "            self.net.add(nn.Conv2D(num_channel, kernel_size=3, \n",
        "                              padding=1, activation='relu'))\n",
        "        self.net.add(nn.MaxPool2D(pool_size=2, strides=2))\n",
        "            \n",
        "    def forward(self, X):\n",
        "        return self.net(X)\n",
        "    \n",
        "def VGG(blocks):\n",
        "    net = nn.Sequential()\n",
        "    # Conv\n",
        "    for num_conv, num_channel in blocks:\n",
        "        net.add(VGGBlock(num_conv, num_channel))\n",
        "    # Dense\n",
        "    net.add(nn.Dense(1024, activation='relu'), nn.Dropout(0.1),\n",
        "             nn.Dense(512, activation='relu'), nn.Dropout(0.1),\n",
        "             nn.Dense(2))\n",
        "    return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e6eBUIHiJbC-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "CtjQsEcvJem-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(net, test_iter, ctx):\n",
        "    acc, n = 0.0, 0\n",
        "    for X, y in test_iter:\n",
        "        X, y = X.as_in_context(ctx), y.astype('float32').as_in_context(ctx)\n",
        "        \n",
        "        y_hat = net(X)\n",
        "        acc += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
        "        n += y.size\n",
        "    return acc / n\n",
        "\n",
        "def train(net, train_iter, test_iter, num_epochs, batch_size, lr, ctx):\n",
        "    \n",
        "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
        "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
        "    \n",
        "    for i in range(1, num_epochs+1):\n",
        "        \n",
        "        train_acc, train_l, n, start = 0.0, 0.0, 0, time.time()\n",
        "        \n",
        "        for X, y in train_iter:\n",
        "            X, y = X.as_in_context(ctx), y.astype('float32').as_in_context(ctx)\n",
        "            \n",
        "            with autograd.record():\n",
        "                y_hat = net(X)\n",
        "                l = loss(y_hat, y).sum()\n",
        "            \n",
        "            l.backward()\n",
        "            trainer.step(batch_size)\n",
        "            train_l += l.asscalar()\n",
        "            train_acc += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
        "            n += y.size\n",
        "            \n",
        "        tm = time.time() - start\n",
        "        print('epoch %s, train loss %.4f, train acc %.4f, time %.2f' % \n",
        "             (i, train_l/n, train_acc/n, tm))\n",
        "        if test_iter:\n",
        "            test_acc = evaluate(net, test_iter, ctx)\n",
        "            print('epoch %s, test acc %.4f.' % (i, test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_kl7GESNJtHa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Experiments"
      ]
    },
    {
      "metadata": {
        "id": "cPf91G_oQ9VK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_epochs, batch_size = 35, 128\n",
        "lr = 5e-2\n",
        "ctx = mx.gpu()\n",
        "\n",
        "train_iter, valid_iter, train_valid_iter, test_iter = load_data_iter(batch_size)\n",
        "# conv_arch = ((1, 32), (1, 64), (2, 128), (2, 256), (2, 256))\n",
        "conv_arch = ((1, 32), (2, 64), (2, 128), (2, 256))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lm_myQLrg9t7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1054
        },
        "outputId": "5faeaa23-cb5e-4981-8c72-d65bea571e75"
      },
      "cell_type": "code",
      "source": [
        "# vgg11 = vgg(conv_arch)\n",
        "# vgg11.initialize(init=init.Xavier(), ctx=ctx)\n",
        "# train(vgg11, train_iter, valid_iter, num_epochs, batch_size, lr, ctx)\n",
        "vgg11 = VGG(conv_arch)\n",
        "vgg11.initialize(init=init.Xavier(), ctx=ctx)\n",
        "train(vgg11, train_iter, valid_iter, num_epochs, batch_size, lr, ctx)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1, train loss 0.6925, train acc 0.5136, time 16.53\n",
            "epoch 1, test acc 0.6093.\n",
            "epoch 2, train loss 0.6907, train acc 0.5369, time 14.16\n",
            "epoch 2, test acc 0.5125.\n",
            "epoch 3, train loss 0.6875, train acc 0.5494, time 14.15\n",
            "epoch 3, test acc 0.5098.\n",
            "epoch 4, train loss 0.6846, train acc 0.5538, time 14.13\n",
            "epoch 4, test acc 0.6068.\n",
            "epoch 5, train loss 0.6783, train acc 0.5769, time 14.20\n",
            "epoch 5, test acc 0.6128.\n",
            "epoch 6, train loss 0.6698, train acc 0.5938, time 14.16\n",
            "epoch 6, test acc 0.5458.\n",
            "epoch 7, train loss 0.6618, train acc 0.6069, time 14.16\n",
            "epoch 7, test acc 0.6401.\n",
            "epoch 8, train loss 0.6471, train acc 0.6288, time 14.26\n",
            "epoch 8, test acc 0.6678.\n",
            "epoch 9, train loss 0.6324, train acc 0.6496, time 14.26\n",
            "epoch 9, test acc 0.6526.\n",
            "epoch 10, train loss 0.6185, train acc 0.6656, time 14.23\n",
            "epoch 10, test acc 0.6783.\n",
            "epoch 11, train loss 0.6037, train acc 0.6750, time 14.36\n",
            "epoch 11, test acc 0.6816.\n",
            "epoch 12, train loss 0.5829, train acc 0.6956, time 14.35\n",
            "epoch 12, test acc 0.7106.\n",
            "epoch 13, train loss 0.5678, train acc 0.7066, time 14.37\n",
            "epoch 13, test acc 0.7264.\n",
            "epoch 14, train loss 0.5514, train acc 0.7186, time 14.52\n",
            "epoch 14, test acc 0.7269.\n",
            "epoch 15, train loss 0.5408, train acc 0.7262, time 14.56\n",
            "epoch 15, test acc 0.7486.\n",
            "epoch 16, train loss 0.5270, train acc 0.7365, time 14.66\n",
            "epoch 16, test acc 0.7226.\n",
            "epoch 17, train loss 0.5055, train acc 0.7497, time 14.63\n",
            "epoch 17, test acc 0.7596.\n",
            "epoch 18, train loss 0.4994, train acc 0.7582, time 14.69\n",
            "epoch 18, test acc 0.7746.\n",
            "epoch 19, train loss 0.4872, train acc 0.7631, time 14.76\n",
            "epoch 19, test acc 0.7781.\n",
            "epoch 20, train loss 0.4679, train acc 0.7748, time 14.76\n",
            "epoch 20, test acc 0.7519.\n",
            "epoch 21, train loss 0.4601, train acc 0.7831, time 14.79\n",
            "epoch 21, test acc 0.7789.\n",
            "epoch 22, train loss 0.4522, train acc 0.7851, time 14.74\n",
            "epoch 22, test acc 0.8029.\n",
            "epoch 23, train loss 0.4359, train acc 0.7937, time 14.75\n",
            "epoch 23, test acc 0.7781.\n",
            "epoch 24, train loss 0.4252, train acc 0.8039, time 14.77\n",
            "epoch 24, test acc 0.7911.\n",
            "epoch 25, train loss 0.4145, train acc 0.8074, time 14.75\n",
            "epoch 25, test acc 0.7919.\n",
            "epoch 26, train loss 0.4002, train acc 0.8167, time 14.74\n",
            "epoch 26, test acc 0.8089.\n",
            "epoch 27, train loss 0.3932, train acc 0.8204, time 14.75\n",
            "epoch 27, test acc 0.8072.\n",
            "epoch 28, train loss 0.3793, train acc 0.8258, time 14.72\n",
            "epoch 28, test acc 0.8249.\n",
            "epoch 29, train loss 0.3682, train acc 0.8309, time 14.71\n",
            "epoch 29, test acc 0.8077.\n",
            "epoch 30, train loss 0.3563, train acc 0.8409, time 14.70\n",
            "epoch 30, test acc 0.8292.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zGa5YHADljmM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pred_result(net, test_iter, ctx, write_path):\n",
        "    res = []\n",
        "    for X, y in test_iter:\n",
        "        X = X.as_in_context(ctx)\n",
        "        y_hat = net(X)\n",
        "        pred  = y_hat.argmax(axis=1).astype('int').asnumpy()\n",
        "        res.extend(pred)\n",
        "        \n",
        "    ids = [x for x in list(range(1, 1+len(test_data)))]\n",
        "    ids.sort(key=lambda x: str(x))\n",
        "    res = [train_valid_data.synsets[x] for x in res]\n",
        "    \n",
        "    res_df = pd.DataFrame({'id':ids, 'label':res})\n",
        "    res_df.to_csv(os.path.join(write_path, 'submission.csv'), index=False)\n",
        "    print('submission file has been saved in:', os.path.join(write_path, 'submission.csv'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nJUh9pvz-KWq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1244
        },
        "outputId": "09b07d32-04b6-412a-99e9-645d01aa39b7"
      },
      "cell_type": "code",
      "source": [
        "vgg11 = VGG(conv_arch)\n",
        "vgg11.initialize(init=init.Xavier(), ctx=ctx)\n",
        "train(vgg11, train_valid_iter, valid_iter, num_epochs, batch_size, lr, ctx)\n",
        "pred_result(vgg11, test_iter, ctx, '/content/gdrive/My Drive/Datasets/dogcat')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1, train loss 0.6921, train acc 0.5156, time 18.46\n",
            "epoch 1, test acc 0.5043.\n",
            "epoch 2, train loss 0.6898, train acc 0.5413, time 18.21\n",
            "epoch 2, test acc 0.6123.\n",
            "epoch 3, train loss 0.6850, train acc 0.5604, time 18.34\n",
            "epoch 3, test acc 0.5718.\n",
            "epoch 4, train loss 0.6759, train acc 0.5779, time 18.39\n",
            "epoch 4, test acc 0.5538.\n",
            "epoch 5, train loss 0.6560, train acc 0.6114, time 18.44\n",
            "epoch 5, test acc 0.6588.\n",
            "epoch 6, train loss 0.6429, train acc 0.6388, time 18.41\n",
            "epoch 6, test acc 0.6741.\n",
            "epoch 7, train loss 0.6287, train acc 0.6476, time 18.43\n",
            "epoch 7, test acc 0.6733.\n",
            "epoch 8, train loss 0.6140, train acc 0.6642, time 18.44\n",
            "epoch 8, test acc 0.6943.\n",
            "epoch 9, train loss 0.5930, train acc 0.6860, time 18.45\n",
            "epoch 9, test acc 0.7304.\n",
            "epoch 10, train loss 0.5737, train acc 0.6989, time 18.36\n",
            "epoch 10, test acc 0.7444.\n",
            "epoch 11, train loss 0.5515, train acc 0.7209, time 18.37\n",
            "epoch 11, test acc 0.7611.\n",
            "epoch 12, train loss 0.5293, train acc 0.7340, time 18.44\n",
            "epoch 12, test acc 0.7181.\n",
            "epoch 13, train loss 0.5116, train acc 0.7472, time 18.50\n",
            "epoch 13, test acc 0.7864.\n",
            "epoch 14, train loss 0.4961, train acc 0.7572, time 18.43\n",
            "epoch 14, test acc 0.7946.\n",
            "epoch 15, train loss 0.4790, train acc 0.7686, time 18.43\n",
            "epoch 15, test acc 0.7924.\n",
            "epoch 16, train loss 0.4661, train acc 0.7772, time 18.42\n",
            "epoch 16, test acc 0.8124.\n",
            "epoch 17, train loss 0.4488, train acc 0.7874, time 18.38\n",
            "epoch 17, test acc 0.8079.\n",
            "epoch 18, train loss 0.4349, train acc 0.7966, time 18.38\n",
            "epoch 18, test acc 0.8177.\n",
            "epoch 19, train loss 0.4189, train acc 0.8068, time 18.48\n",
            "epoch 19, test acc 0.8404.\n",
            "epoch 20, train loss 0.4052, train acc 0.8140, time 18.45\n",
            "epoch 20, test acc 0.8369.\n",
            "epoch 21, train loss 0.3932, train acc 0.8198, time 18.50\n",
            "epoch 21, test acc 0.8567.\n",
            "epoch 22, train loss 0.3784, train acc 0.8297, time 18.46\n",
            "epoch 22, test acc 0.8557.\n",
            "epoch 23, train loss 0.3630, train acc 0.8350, time 18.42\n",
            "epoch 23, test acc 0.8689.\n",
            "epoch 24, train loss 0.3485, train acc 0.8465, time 18.47\n",
            "epoch 24, test acc 0.8822.\n",
            "epoch 25, train loss 0.3325, train acc 0.8553, time 18.49\n",
            "epoch 25, test acc 0.8667.\n",
            "epoch 26, train loss 0.3252, train acc 0.8587, time 18.43\n",
            "epoch 26, test acc 0.8847.\n",
            "epoch 27, train loss 0.3083, train acc 0.8660, time 18.45\n",
            "epoch 27, test acc 0.9030.\n",
            "epoch 28, train loss 0.3026, train acc 0.8680, time 18.47\n",
            "epoch 28, test acc 0.8877.\n",
            "epoch 29, train loss 0.2771, train acc 0.8801, time 18.40\n",
            "epoch 29, test acc 0.9122.\n",
            "epoch 30, train loss 0.2669, train acc 0.8868, time 18.42\n",
            "epoch 30, test acc 0.9237.\n",
            "epoch 31, train loss 0.2579, train acc 0.8905, time 18.39\n",
            "epoch 31, test acc 0.9285.\n",
            "epoch 32, train loss 0.2347, train acc 0.9018, time 18.37\n",
            "epoch 32, test acc 0.9245.\n",
            "epoch 33, train loss 0.2568, train acc 0.8927, time 18.34\n",
            "epoch 33, test acc 0.9405.\n",
            "epoch 34, train loss 0.2148, train acc 0.9129, time 18.46\n",
            "epoch 34, test acc 0.9457.\n",
            "epoch 35, train loss 0.1963, train acc 0.9191, time 18.42\n",
            "epoch 35, test acc 0.9347.\n",
            "submission file has been saved in: /content/gdrive/My Drive/Datasets/dogcat/submission.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uOjZCrG95uhY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define ResNet"
      ]
    },
    {
      "metadata": {
        "id": "jFHRB3HH5IR5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ResBlock(nn.Block):\n",
        "    def __init__(self, n_channels, strides=1, nin=False, **kwargs):\n",
        "        super(ResBlock, self).__init__(**kwargs)\n",
        "        \n",
        "        self.conv1 = nn.Conv2D(n_channels, kernel_size=3, padding=1, strides=strides)\n",
        "        self.conv2 = nn.Conv2D(n_channels, kernel_size=3, padding=1)\n",
        "        \n",
        "        self.nin   = None\n",
        "        if nin:\n",
        "            self.nin = nn.Conv2D(n_channels, kernel_size=1, strides=strides)\n",
        "            \n",
        "    def forward(self, X):\n",
        "        res = nd.relu(self.conv1(X))\n",
        "        res = self.conv2(res)\n",
        "        \n",
        "        if self.nin:\n",
        "            X = self.nin(X)\n",
        "            \n",
        "        return nd.relu(res + X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gQj4IuPO7VWX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ResNet18(nn.Block):\n",
        "    \n",
        "    def __init__(self, n_outputs, **kwargs):\n",
        "        super(ResNet18, self).__init__(**kwargs)\n",
        "        \n",
        "        self.net = nn.Sequential()\n",
        "        self.net.add(\n",
        "            nn.Conv2D(64, kernel_size=3, strides=1, padding=1),\n",
        "            nn.BatchNorm(),\n",
        "            nn.Activation('relu')\n",
        "        )\n",
        "        \n",
        "        self.net.add(\n",
        "            self.add_block(64, 2, nin=True),\n",
        "            self.add_block(128, 2),\n",
        "            self.add_block(256, 2),\n",
        "            self.add_block(512, 2)\n",
        "        )\n",
        "        \n",
        "        self.net.add(nn.GlobalAvgPool2D(), nn.Dense(n_outputs))\n",
        "        \n",
        "    def forward(self, X):\n",
        "        return self.net(X)\n",
        "        \n",
        "    def add_block(self, n_channels, n_blocks, nin=False):\n",
        "        net = nn.Sequential()\n",
        "\n",
        "        if not nin:\n",
        "            net.add(ResBlock(n_channels, 2, True))\n",
        "            n_blocks -= 1\n",
        "\n",
        "        for i in range(n_blocks):\n",
        "            net.add(ResBlock(n_channels))\n",
        "\n",
        "        return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GHRr6bm4J5Ej",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# resnet = ResNet18(2)\n",
        "# resnet.initialize(init=init.Xavier(), force_reinit=True, ctx=ctx)\n",
        "train(vgg11, train_iter, valid_iter, 5, batch_size, 1e-3, ctx)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}