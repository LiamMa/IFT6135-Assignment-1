{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1_3_Jin.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "1eGIV7SLuBhP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Environment Setting and Data Preprocessing\n",
        "\n",
        "To reproduce the result, please put the dataset into a folder of google drive whose path is saved in *data_path*,  in our case, data_path = '/content/gdrive/My\\ Drive/Datasets/dogcat'"
      ]
    },
    {
      "metadata": {
        "id": "W9Scb4r5XYt3",
        "colab_type": "code",
        "outputId": "41268f65-72bb-40f2-c114-8874e2e7085e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        }
      },
      "cell_type": "code",
      "source": [
        "! nvidia-smi\n",
        "! pip install mxnet-cu100"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Feb 11 19:24:06 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 410.79       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    68W / 149W |   1044MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Requirement already satisfied: mxnet-cu100 in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (2.21.0)\n",
            "Requirement already satisfied: numpy<1.15.0,>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (1.14.6)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (0.8.4)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (1.22)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (2018.11.29)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z7yPuUVEwMGd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import mxnet as mx\n",
        "from mxnet import autograd, nd, init, gluon\n",
        "from mxnet.gluon import nn, loss as gloss, data as gdata\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import zipfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OQTNJuotEurB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We load the dataset and move files into the virtual machine environment of colab\n",
        " to speed up image loading."
      ]
    },
    {
      "metadata": {
        "id": "vk9K9HmXjiFm",
        "colab_type": "code",
        "outputId": "69c2b46e-f234-4538-8fef-10d596a9316d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "data_path = '/content/gdrive/My\\ Drive/Datasets/dogcat'\n",
        "base_path = '/content/dogcat'\n",
        "\n",
        "! rm -rf '/content/dogcat'\n",
        "! cp -r $data_path /content\n",
        "\n",
        "for f in ['trainset.zip', 'testset.zip']:\n",
        "    with zipfile.ZipFile(os.path.join(base_path, f)) as z:\n",
        "        z.extractall(base_path)\n",
        "        \n",
        "! rm -rf /content/dogcat/__MACOSX\n",
        "! ls /content/dogcat"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "sample_submission.csv  testset\t    trainset\n",
            "submission.csv\t       testset.zip  trainset.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uVDMOat-E4p4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We divided the dataset into four folders:\n",
        "\n",
        "- trainset: the original training dataset which will be used to re-train the final model after parameter tuning.\n",
        "- testset: the original test dataset used to generate predictions.\n",
        "- train, valid: the training dataset and validation dataset used to train and select hyperparameters. They are splitted from trainset as a split ratio."
      ]
    },
    {
      "metadata": {
        "id": "H_4IhW_6w69l",
        "colab_type": "code",
        "outputId": "83d8ed50-452a-4ee8-94d5-2d4045b42047",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "train_valid_path = 'trainset'\n",
        "test_path = 'testset'\n",
        "train_path = 'train'\n",
        "valid_path = 'valid'\n",
        "\n",
        "\n",
        "def delete_dir(path):\n",
        "    if os.path.exists(os.path.join(*path)):\n",
        "        shutil.rmtree(os.path.join(*path))\n",
        "        \n",
        "def create_dir(path):\n",
        "    if not os.path.exists(os.path.join(*path)):\n",
        "        os.makedirs(os.path.join(*path))\n",
        "\n",
        "def devide_train_valid(base_path, train_valid_path, train_path, valid_path,\n",
        "                       valid_ratio=0.2, labels=['Dog', 'Cat']):\n",
        "    delete_dir([base_path, train_path])\n",
        "    delete_dir([base_path, valid_path])\n",
        "        \n",
        "    all_files_by_label = [os.listdir(os.path.join(base_path, train_valid_path, label))\n",
        "                          for label in labels]\n",
        "    \n",
        "    count_train = {label:0 for label in labels}\n",
        "    count_valid = {label:0 for label in labels}\n",
        "    \n",
        "    for label, file_per_label in zip(labels, all_files_by_label):\n",
        "        print('Train&Valid set, %s: %s' % (label, len(file_per_label)))\n",
        "        create_dir([base_path, train_path, label])\n",
        "        create_dir([base_path, valid_path, label])\n",
        "        for f in file_per_label:\n",
        "            if count_train[label] < len(file_per_label) * (1 - valid_ratio):\n",
        "                shutil.copy(os.path.join(base_path, train_valid_path, label, f),\n",
        "                            os.path.join(base_path, train_path, label, f))\n",
        "                count_train[label] += 1\n",
        "            else:\n",
        "                shutil.copy(os.path.join(base_path, train_valid_path, label, f),\n",
        "                            os.path.join(base_path, valid_path, label, f))\n",
        "                count_valid[label] += 1\n",
        "        print('Train set, %s: %s' % (label, count_train[label]))\n",
        "        print('Valid set, %s: %s' % (label, count_valid[label]))\n",
        "                \n",
        "devide_train_valid(base_path, train_valid_path, train_path, valid_path)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train&Valid set, Dog: 9999\n",
            "Train set, Dog: 8000\n",
            "Valid set, Dog: 1999\n",
            "Train&Valid set, Cat: 9999\n",
            "Train set, Cat: 8000\n",
            "Valid set, Cat: 1999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bF8LAVpLFscM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We defined four data iterators as below, given a batch size. We also have different data augmentation pipelines for train dataset and test dataset, since test dataset should keep unchanged."
      ]
    },
    {
      "metadata": {
        "id": "SsDH1T9pwG3B",
        "colab_type": "code",
        "outputId": "212f0f67-da91-4df5-cf81-12573725969b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "train_data = gdata.vision.ImageFolderDataset(\n",
        "    os.path.join(base_path, train_path), flag=1)\n",
        "valid_data = gdata.vision.ImageFolderDataset(\n",
        "    os.path.join(base_path, valid_path), flag=1)\n",
        "train_valid_data = gdata.vision.ImageFolderDataset(\n",
        "    os.path.join(base_path, train_valid_path), flag=1)\n",
        "test_data = gdata.vision.ImageFolderDataset(\n",
        "    os.path.join(base_path, test_path), flag=1)\n",
        "\n",
        "aug_train = gdata.vision.transforms.Compose([\n",
        "    gdata.vision.transforms.RandomResizedCrop(64, scale=(0.75, 1),\n",
        "                                               ratio=(3.0/4.0, 4.0/3.0)),\n",
        "        gdata.vision.transforms.RandomFlipLeftRight(),\n",
        "    gdata.vision.transforms.RandomColorJitter(brightness=0.4, \n",
        "                                              contrast=0.4, saturation=0.4),\n",
        "    gdata.vision.transforms.RandomLighting(0.1),\n",
        "    gdata.vision.transforms.ToTensor(),\n",
        "#     gdata.vision.transforms.Normalize([0.485, 0.456, 0.406], \n",
        "#                                       [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "aug_test = gdata.vision.transforms.Compose([\n",
        "    gdata.vision.transforms.ToTensor(),\n",
        "#     gdata.vision.transforms.Normalize([0.485, 0.456, 0.406], \n",
        "#                                       [0.229, 0.224, 0.225])\n",
        "])\n",
        "    \n",
        "def load_data_iter(batch_size):\n",
        "\n",
        "    train_iter = gdata.DataLoader(train_data.transform_first(aug_train), batch_size,\n",
        "                                  shuffle=True, last_batch='keep')\n",
        "    valid_iter = gdata.DataLoader(valid_data.transform_first(aug_test), batch_size,\n",
        "                                  shuffle=True, last_batch='keep')\n",
        "    train_valid_iter = gdata.DataLoader(train_valid_data.transform_first(aug_train), batch_size,\n",
        "                                  shuffle=True, last_batch='keep')\n",
        "    test_iter  = gdata.DataLoader(test_data.transform_first(aug_test), batch_size,\n",
        "                                  shuffle=False, last_batch='keep')\n",
        "    \n",
        "    return train_iter, valid_iter, train_valid_iter, test_iter"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Ignoring /content/dogcat/trainset/.DS_Store, which is not a directory.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Vk6fUe-Tn0lI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building VGG Model"
      ]
    },
    {
      "metadata": {
        "id": "GNviaRqLO7U2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# VGG modified\n",
        "class VGGBlock(nn.Block):\n",
        "    def __init__(self, num_conv, num_channel, **kwargs):\n",
        "        super(VGGBlock, self).__init__(**kwargs)\n",
        "        \n",
        "        self.net = nn.Sequential()\n",
        "        for i in range(num_conv):\n",
        "            self.net.add(nn.Conv2D(num_channel, kernel_size=3, \n",
        "                              padding=1, activation='relu'))\n",
        "        self.net.add(nn.MaxPool2D(pool_size=2, strides=2))\n",
        "            \n",
        "    def forward(self, X):\n",
        "        return self.net(X)\n",
        "    \n",
        "def VGG(blocks):\n",
        "    net = nn.Sequential()\n",
        "    # Conv\n",
        "    for num_conv, num_channel in blocks:\n",
        "        net.add(VGGBlock(num_conv, num_channel))\n",
        "    # Dense\n",
        "    net.add(nn.Dense(1024, activation='relu'), nn.Dropout(0.0),\n",
        "             nn.Dense(512, activation='relu'), nn.Dropout(0.0),\n",
        "             nn.Dense(2))\n",
        "    return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e6eBUIHiJbC-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "CtjQsEcvJem-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(net, test_iter, ctx):\n",
        "    acc, n = 0.0, 0\n",
        "    for X, y in test_iter:\n",
        "        X, y = X.as_in_context(ctx), y.astype('float32').as_in_context(ctx)\n",
        "        \n",
        "        y_hat = net(X)\n",
        "        acc += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
        "        n += y.size\n",
        "    return acc / n\n",
        "\n",
        "def train(net, train_iter, test_iter, num_epochs, batch_size, lr, ctx):\n",
        "    \n",
        "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
        "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
        "    \n",
        "    for i in range(1, num_epochs+1):\n",
        "        \n",
        "        train_acc, train_l, n, start = 0.0, 0.0, 0, time.time()\n",
        "        \n",
        "        for X, y in train_iter:\n",
        "            X, y = X.as_in_context(ctx), y.astype('float32').as_in_context(ctx)\n",
        "            \n",
        "            with autograd.record():\n",
        "                y_hat = net(X)\n",
        "                l = loss(y_hat, y).sum()\n",
        "            \n",
        "            l.backward()\n",
        "            trainer.step(batch_size)\n",
        "            train_l += l.asscalar()\n",
        "            train_acc += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
        "            n += y.size\n",
        "            \n",
        "        tm = time.time() - start\n",
        "        print('epoch %s, train loss %.4f, train acc %.4f, time %.2f' % \n",
        "             (i, train_l/n, train_acc/n, tm))\n",
        "        if test_iter:\n",
        "            test_acc = evaluate(net, test_iter, ctx)\n",
        "            print('epoch %s, test acc %.4f.' % (i, test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_kl7GESNJtHa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Experiments"
      ]
    },
    {
      "metadata": {
        "id": "cPf91G_oQ9VK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_epochs, batch_size = 35, 128\n",
        "lr = 5e-2\n",
        "ctx = mx.gpu()\n",
        "\n",
        "train_iter, valid_iter, train_valid_iter, test_iter = load_data_iter(batch_size)\n",
        "# conv_arch = ((1, 32), (1, 64), (2, 128), (2, 256), (2, 256))\n",
        "conv_arch = ((1, 32), (2, 64), (2, 128), (2, 256))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lm_myQLrg9t7",
        "colab_type": "code",
        "outputId": "bbb024fa-ad12-4210-9cc4-c9cfe886fb7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1226
        }
      },
      "cell_type": "code",
      "source": [
        "# vgg11 = vgg(conv_arch)\n",
        "# vgg11.initialize(init=init.Xavier(), ctx=ctx)\n",
        "# train(vgg11, train_iter, valid_iter, num_epochs, batch_size, lr, ctx)\n",
        "vgg11 = VGG(conv_arch)\n",
        "vgg11.initialize(init=init.Xavier(), ctx=ctx)\n",
        "train(vgg11, train_iter, valid_iter, num_epochs, batch_size, lr, ctx)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1, train loss 0.6924, train acc 0.5152, time 19.20\n",
            "epoch 1, test acc 0.5015.\n",
            "epoch 2, train loss 0.6910, train acc 0.5321, time 16.99\n",
            "epoch 2, test acc 0.5468.\n",
            "epoch 3, train loss 0.6873, train acc 0.5513, time 17.88\n",
            "epoch 3, test acc 0.5253.\n",
            "epoch 4, train loss 0.6840, train acc 0.5618, time 18.41\n",
            "epoch 4, test acc 0.5733.\n",
            "epoch 5, train loss 0.6808, train acc 0.5706, time 18.26\n",
            "epoch 5, test acc 0.5628.\n",
            "epoch 6, train loss 0.6767, train acc 0.5806, time 18.39\n",
            "epoch 6, test acc 0.6178.\n",
            "epoch 7, train loss 0.6689, train acc 0.5980, time 18.37\n",
            "epoch 7, test acc 0.6313.\n",
            "epoch 8, train loss 0.6603, train acc 0.6118, time 18.24\n",
            "epoch 8, test acc 0.6406.\n",
            "epoch 9, train loss 0.6546, train acc 0.6236, time 18.23\n",
            "epoch 9, test acc 0.6311.\n",
            "epoch 10, train loss 0.6442, train acc 0.6358, time 17.62\n",
            "epoch 10, test acc 0.6716.\n",
            "epoch 11, train loss 0.6301, train acc 0.6508, time 17.55\n",
            "epoch 11, test acc 0.6928.\n",
            "epoch 12, train loss 0.6228, train acc 0.6562, time 17.49\n",
            "epoch 12, test acc 0.6578.\n",
            "epoch 13, train loss 0.6126, train acc 0.6688, time 17.56\n",
            "epoch 13, test acc 0.6931.\n",
            "epoch 14, train loss 0.5970, train acc 0.6826, time 17.43\n",
            "epoch 14, test acc 0.7204.\n",
            "epoch 15, train loss 0.5858, train acc 0.6872, time 17.39\n",
            "epoch 15, test acc 0.7174.\n",
            "epoch 16, train loss 0.5696, train acc 0.7041, time 17.45\n",
            "epoch 16, test acc 0.7354.\n",
            "epoch 17, train loss 0.5591, train acc 0.7131, time 17.43\n",
            "epoch 17, test acc 0.7386.\n",
            "epoch 18, train loss 0.5463, train acc 0.7247, time 17.74\n",
            "epoch 18, test acc 0.7306.\n",
            "epoch 19, train loss 0.5377, train acc 0.7226, time 17.91\n",
            "epoch 19, test acc 0.7589.\n",
            "epoch 20, train loss 0.5166, train acc 0.7432, time 18.24\n",
            "epoch 20, test acc 0.7639.\n",
            "epoch 21, train loss 0.5132, train acc 0.7449, time 17.92\n",
            "epoch 21, test acc 0.6941.\n",
            "epoch 22, train loss 0.5050, train acc 0.7496, time 17.92\n",
            "epoch 22, test acc 0.7816.\n",
            "epoch 23, train loss 0.4981, train acc 0.7542, time 17.89\n",
            "epoch 23, test acc 0.7521.\n",
            "epoch 24, train loss 0.4824, train acc 0.7652, time 17.87\n",
            "epoch 24, test acc 0.7621.\n",
            "epoch 25, train loss 0.4748, train acc 0.7731, time 17.94\n",
            "epoch 25, test acc 0.7906.\n",
            "epoch 26, train loss 0.4687, train acc 0.7772, time 17.55\n",
            "epoch 26, test acc 0.8037.\n",
            "epoch 27, train loss 0.4584, train acc 0.7835, time 17.61\n",
            "epoch 27, test acc 0.7886.\n",
            "epoch 28, train loss 0.4516, train acc 0.7834, time 17.71\n",
            "epoch 28, test acc 0.7996.\n",
            "epoch 29, train loss 0.4395, train acc 0.7934, time 17.67\n",
            "epoch 29, test acc 0.7886.\n",
            "epoch 30, train loss 0.4310, train acc 0.7964, time 17.54\n",
            "epoch 30, test acc 0.8087.\n",
            "epoch 31, train loss 0.4226, train acc 0.8019, time 17.64\n",
            "epoch 31, test acc 0.8179.\n",
            "epoch 32, train loss 0.4219, train acc 0.8042, time 17.31\n",
            "epoch 32, test acc 0.8027.\n",
            "epoch 33, train loss 0.4078, train acc 0.8119, time 17.30\n",
            "epoch 33, test acc 0.8259.\n",
            "epoch 34, train loss 0.3986, train acc 0.8182, time 17.89\n",
            "epoch 34, test acc 0.8289.\n",
            "epoch 35, train loss 0.3935, train acc 0.8194, time 18.02\n",
            "epoch 35, test acc 0.8192.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zGa5YHADljmM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pred_result(net, test_iter, ctx, write_path):\n",
        "    res = []\n",
        "    for X, y in test_iter:\n",
        "        X = X.as_in_context(ctx)\n",
        "        y_hat = net(X)\n",
        "        pred  = y_hat.argmax(axis=1).astype('int').asnumpy()\n",
        "        res.extend(pred)\n",
        "        \n",
        "    ids = [x for x in list(range(1, 1+len(test_data)))]\n",
        "    ids.sort(key=lambda x: str(x))\n",
        "    res = [train_valid_data.synsets[x] for x in res]\n",
        "    \n",
        "    res_df = pd.DataFrame({'id':ids, 'label':res})\n",
        "    res_df.to_csv(os.path.join(write_path, 'submission.csv'), index=False)\n",
        "    print('submission file has been saved in:', os.path.join(write_path, 'submission.csv'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nJUh9pvz-KWq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vgg11 = VGG(conv_arch)\n",
        "vgg11.initialize(init=init.Xavier(), ctx=ctx)\n",
        "train(vgg11, train_valid_iter, valid_iter, num_epochs, batch_size, lr, ctx)\n",
        "pred_result(vgg11, test_iter, ctx, '/content/gdrive/My Drive/Datasets/dogcat')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uOjZCrG95uhY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define ResNet"
      ]
    },
    {
      "metadata": {
        "id": "jFHRB3HH5IR5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ResBlock(nn.Block):\n",
        "    def __init__(self, n_channels, strides=1, nin=False, **kwargs):\n",
        "        super(ResBlock, self).__init__(**kwargs)\n",
        "        \n",
        "        self.conv1 = nn.Conv2D(n_channels, kernel_size=3, padding=1, strides=strides)\n",
        "        self.conv2 = nn.Conv2D(n_channels, kernel_size=3, padding=1)\n",
        "        \n",
        "        self.nin   = None\n",
        "        if nin:\n",
        "            self.nin = nn.Conv2D(n_channels, kernel_size=1, strides=strides)\n",
        "            \n",
        "        self.bn1 = nn.BatchNorm()\n",
        "        self.bn2 = nn.BatchNorm()\n",
        "            \n",
        "    def forward(self, X):\n",
        "        res = nd.relu(self.bn1(self.conv1(X)))\n",
        "        res = self.bn2(self.conv2(res))\n",
        "        \n",
        "        if self.nin:\n",
        "            X = self.nin(X)\n",
        "        return nd.relu(res + X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gQj4IuPO7VWX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ResNet18(nn.Block):\n",
        "    \n",
        "    def __init__(self, n_outputs, **kwargs):\n",
        "        super(ResNet18, self).__init__(**kwargs)\n",
        "        \n",
        "        self.net = nn.Sequential()\n",
        "        self.net.add(\n",
        "            nn.Conv2D(64, kernel_size=3, strides=1, padding=1),\n",
        "            nn.BatchNorm(),\n",
        "            nn.Activation('relu')\n",
        "        )\n",
        "        \n",
        "        self.net.add(\n",
        "            self.add_block(64, 2, nin=True),\n",
        "            self.add_block(128, 2),\n",
        "            self.add_block(256, 2),\n",
        "            self.add_block(512, 2)\n",
        "        )\n",
        "        \n",
        "        self.net.add(nn.GlobalAvgPool2D(), nn.Dense(n_outputs))\n",
        "        \n",
        "    def forward(self, X):\n",
        "        return self.net(X)\n",
        "        \n",
        "    def add_block(self, n_channels, n_blocks, nin=False):\n",
        "        net = nn.Sequential()\n",
        "\n",
        "        if not nin:\n",
        "            net.add(ResBlock(n_channels, 2, True))\n",
        "            n_blocks -= 1\n",
        "\n",
        "        for i in range(n_blocks):\n",
        "            net.add(ResBlock(n_channels))\n",
        "\n",
        "        return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GHRr6bm4J5Ej",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1226
        },
        "outputId": "9aaf87b6-a899-4a95-e7e0-a3f0c071975b"
      },
      "cell_type": "code",
      "source": [
        "resnet = ResNet18(2)\n",
        "resnet.initialize(init=init.Xavier(), force_reinit=True, ctx=ctx)\n",
        "num_epochs, batch_size = 35, 128\n",
        "lr = 5e-2\n",
        "ctx = mx.gpu()\n",
        "train(resnet, train_iter, valid_iter, num_epochs, batch_size, lr, ctx)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1, train loss 0.9074, train acc 0.5543, time 97.63\n",
            "epoch 1, test acc 0.5458.\n",
            "epoch 2, train loss 0.6675, train acc 0.6156, time 97.46\n",
            "epoch 2, test acc 0.6496.\n",
            "epoch 3, train loss 0.6459, train acc 0.6331, time 97.36\n",
            "epoch 3, test acc 0.5128.\n",
            "epoch 4, train loss 0.6149, train acc 0.6719, time 97.23\n",
            "epoch 4, test acc 0.6738.\n",
            "epoch 5, train loss 0.5932, train acc 0.6843, time 97.23\n",
            "epoch 5, test acc 0.7404.\n",
            "epoch 6, train loss 0.5673, train acc 0.7096, time 97.54\n",
            "epoch 6, test acc 0.7071.\n",
            "epoch 7, train loss 0.5462, train acc 0.7221, time 97.28\n",
            "epoch 7, test acc 0.5630.\n",
            "epoch 8, train loss 0.5244, train acc 0.7425, time 97.36\n",
            "epoch 8, test acc 0.7449.\n",
            "epoch 9, train loss 0.4928, train acc 0.7650, time 97.67\n",
            "epoch 9, test acc 0.7224.\n",
            "epoch 10, train loss 0.4694, train acc 0.7756, time 97.37\n",
            "epoch 10, test acc 0.6308.\n",
            "epoch 11, train loss 0.4428, train acc 0.7924, time 97.41\n",
            "epoch 11, test acc 0.7759.\n",
            "epoch 12, train loss 0.4149, train acc 0.8088, time 97.21\n",
            "epoch 12, test acc 0.6983.\n",
            "epoch 13, train loss 0.3889, train acc 0.8205, time 97.21\n",
            "epoch 13, test acc 0.8462.\n",
            "epoch 14, train loss 0.3670, train acc 0.8356, time 97.23\n",
            "epoch 14, test acc 0.8549.\n",
            "epoch 15, train loss 0.3516, train acc 0.8462, time 97.04\n",
            "epoch 15, test acc 0.8382.\n",
            "epoch 16, train loss 0.3305, train acc 0.8558, time 97.04\n",
            "epoch 16, test acc 0.8024.\n",
            "epoch 17, train loss 0.3216, train acc 0.8576, time 97.34\n",
            "epoch 17, test acc 0.7831.\n",
            "epoch 18, train loss 0.3093, train acc 0.8628, time 96.94\n",
            "epoch 18, test acc 0.8344.\n",
            "epoch 19, train loss 0.2951, train acc 0.8726, time 96.91\n",
            "epoch 19, test acc 0.8232.\n",
            "epoch 20, train loss 0.2825, train acc 0.8768, time 97.07\n",
            "epoch 20, test acc 0.8769.\n",
            "epoch 21, train loss 0.2803, train acc 0.8781, time 96.76\n",
            "epoch 21, test acc 0.8759.\n",
            "epoch 22, train loss 0.2631, train acc 0.8876, time 96.71\n",
            "epoch 22, test acc 0.7274.\n",
            "epoch 23, train loss 0.2562, train acc 0.8912, time 96.86\n",
            "epoch 23, test acc 0.8889.\n",
            "epoch 24, train loss 0.2475, train acc 0.8909, time 96.57\n",
            "epoch 24, test acc 0.8907.\n",
            "epoch 25, train loss 0.2411, train acc 0.8955, time 96.79\n",
            "epoch 25, test acc 0.8414.\n",
            "epoch 26, train loss 0.2297, train acc 0.9010, time 96.81\n",
            "epoch 26, test acc 0.8729.\n",
            "epoch 27, train loss 0.2206, train acc 0.9078, time 96.76\n",
            "epoch 27, test acc 0.8609.\n",
            "epoch 28, train loss 0.2219, train acc 0.9074, time 96.71\n",
            "epoch 28, test acc 0.8822.\n",
            "epoch 29, train loss 0.2104, train acc 0.9082, time 96.79\n",
            "epoch 29, test acc 0.8872.\n",
            "epoch 30, train loss 0.2125, train acc 0.9116, time 96.71\n",
            "epoch 30, test acc 0.8734.\n",
            "epoch 31, train loss 0.1979, train acc 0.9131, time 96.68\n",
            "epoch 31, test acc 0.8919.\n",
            "epoch 32, train loss 0.1928, train acc 0.9181, time 96.84\n",
            "epoch 32, test acc 0.9155.\n",
            "epoch 33, train loss 0.1897, train acc 0.9206, time 96.84\n",
            "epoch 33, test acc 0.8724.\n",
            "epoch 34, train loss 0.1828, train acc 0.9219, time 96.64\n",
            "epoch 34, test acc 0.9150.\n",
            "epoch 35, train loss 0.1771, train acc 0.9261, time 96.91\n",
            "epoch 35, test acc 0.9135.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "T0qs5K40BaPO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "ddec7b1c-ede8-480a-c4b5-b5060c50eab6"
      },
      "cell_type": "code",
      "source": [
        "for X, y in train_iter:\n",
        "    X = X.as_in_context(mx.gpu())\n",
        "    print(X.shape)        \n",
        "    X = resnet(X)\n",
        "    print(X.shape)\n",
        "        \n",
        "    break"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128, 3, 64, 64)\n",
            "(128, 64, 64, 64)\n",
            "(128, 64, 64, 64)\n",
            "(128, 128, 32, 32)\n",
            "(128, 128, 32, 32)\n",
            "(128, 256, 16, 16)\n",
            "(128, 256, 16, 16)\n",
            "(128, 512, 8, 8)\n",
            "(128, 512, 8, 8)\n",
            "(128, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RNlu-Y48B3LI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}