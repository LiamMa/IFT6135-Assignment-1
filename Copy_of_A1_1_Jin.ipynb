{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of A1_1_Jin.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/djdongjin/IFT6135-Assignment/blob/master/Copy_of_A1_1_Jin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ajrMnZLJeMHv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "np.random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IgyWZVSD64lB",
        "colab_type": "code",
        "outputId": "e1501226-e1cb-45dc-eab6-5fe38a64cf29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HN603fuc-9PX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def one_hot(labels, n):\n",
        "    \"\"\"labels: m*1 vector\n",
        "       n: expected classes\n",
        "       outout: m*n matrix\"\"\"\n",
        "    m = len(labels)\n",
        "    onehot = np.zeros((m, n))\n",
        "    onehot[np.arange(m), labels] = 1\n",
        "    return onehot\n",
        "\n",
        "DATA_PATH = r'/content/gdrive/My Drive/app/MNIST'\n",
        "X_train = np.load(DATA_PATH + '/x_train.npy')\n",
        "y_train = one_hot(np.load(DATA_PATH + '/y_train.npy'),10)\n",
        "X_val   = np.load(DATA_PATH + '/x_val.npy')\n",
        "y_val   = one_hot(np.load(DATA_PATH + '/y_val.npy'),10)\n",
        "X_test  = np.load(DATA_PATH + '/x_test.npy')\n",
        "y_test  = one_hot(np.load(DATA_PATH + '/y_test.npy'),10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BXmeo9zw7Swx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def accuracy(y_pred, y):\n",
        "    return np.sum(np.argmax(y_pred, axis=1) == np.argmax(y, axis=1)) / y.shape[0]\n",
        "\n",
        "\n",
        "def data_iter(data, batch_size):\n",
        "    X, y = data\n",
        "    batches = [(X[i:i+batch_size],y[i:i+batch_size]) for i in range(0,X.shape[0],batch_size)]\n",
        "    random.shuffle(batches)\n",
        "    for batch in batches:\n",
        "        yield batch\n",
        "                \n",
        "        \n",
        "def glorot(in_dim, out_dim):\n",
        "    d = np.sqrt(6/(in_dim+out_dim))\n",
        "    return np.random.normal(-d,d,(in_dim,out_dim))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UONENIM-fV0l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "INPUT_DIM = 784\n",
        "OUTPUT_DIM = 10\n",
        "\n",
        "class NN(object):\n",
        "    \n",
        "    \n",
        "    def __init__(self,hidden_dims=[1024,2048],n_hidden=2,init='Normal',activate='relu',mode='train'):\n",
        "        self.dims = [INPUT_DIM,] + hidden_dims + [OUTPUT_DIM,]\n",
        "        self.weights = []\n",
        "        self.biases  = []\n",
        "        self.init = init\n",
        "        self.activate = activate\n",
        "        \n",
        "        self.initialize_weights(n_hidden, self.dims)\n",
        "        \n",
        "        \n",
        "    def initialize_weights(self, n_hidden, dims):\n",
        "        init_method = None\n",
        "        if self.init == 'Zero':\n",
        "            init_method = lambda x, y: np.zeros(x,y)\n",
        "        elif self.init == 'Normal':\n",
        "            init_method = lambda x, y: np.random.randn(x,y)\n",
        "        elif self.init == 'Glorot':\n",
        "            init_method = glorot\n",
        "            \n",
        "        for (inputs, outputs) in zip(dims[:-1], dims[1:]):\n",
        "            self.weights.append(init_method(inputs, outputs))\n",
        "            self.biases.append(np.zeros(outputs))\n",
        "            \n",
        "            \n",
        "    def activation(self,inputs):\n",
        "        if self.activate == 'relu':\n",
        "            inputs[inputs < 0] = 0\n",
        "            return inputs\n",
        "        if self.activate == 'sigmoid':\n",
        "            return 1.0/(1.0+np.exp(-inputs))\n",
        "            \n",
        "            \n",
        "    def forward(self, inputs, labels):\n",
        "        a_k = None\n",
        "        h_k = inputs\n",
        "        a = []\n",
        "        h = [h_k]\n",
        "        for (W, b) in zip(self.weights[:-1], self.biases[:-1]):\n",
        "            a_k = np.dot(h_k, W) + b\n",
        "            h_k = self.activation(a_k)\n",
        "            a.append(a_k)\n",
        "            h.append(h_k)\n",
        "        \n",
        "        a_k = np.dot(h_k, self.weights[-1]) + self.biases[-1]\n",
        "        h_k = self.softmax(a_k)\n",
        "        a.append(a_k)\n",
        "        h.append(h_k)\n",
        "        \n",
        "        ls = self.loss(h_k, labels)\n",
        "        cache = (a, h)\n",
        "        \n",
        "        return h_k, ls, cache\n",
        "    \n",
        "    \n",
        "    \n",
        "    def loss(self, pred, labels):\n",
        "        '''\n",
        "        Negative log likelihood\n",
        "        '''\n",
        "        ls = np.nan_to_num(np.log(pred))\n",
        "        ls = - np.sum(labels * ls, axis=1)\n",
        "        return ls\n",
        "    \n",
        "    \n",
        "    def backward(self,cache,labels,lss):\n",
        "        \"\"\"\n",
        "        Input: cache: (as, hs)\n",
        "                    as: preactivate values\n",
        "                    hs: activated values\n",
        "                    lss: loss for each examples\n",
        "        output: grads: (grads_w, grads_b)\n",
        "        \"\"\"\n",
        "        n_w = [np.zeros_like(w) for w in self.weights]\n",
        "        n_b = [np.zeros_like(b) for b in self.biases]\n",
        "        as_ = cache[0]\n",
        "        hs_ = cache[1]\n",
        "        for i in range(labels.shape[0]):\n",
        "            nabla_w = [np.zeros_like(w) for w in self.weights]\n",
        "            nabla_b = [np.zeros_like(b) for b in self.biases]\n",
        "            a = [aa[i] for aa in as_]\n",
        "            h = [hh[i] for hh in hs_]\n",
        "            ls = lss[i]\n",
        "            label = labels[i,:]\n",
        "            \n",
        "            # nabla l -> softmax -> pre-softmax\n",
        "            nabla_a = -(label - h[-1])\n",
        "            nabla_b[-1] = nabla_a\n",
        "            nabla_w[-1] = np.outer(h[-2], nabla_a)\n",
        "            # for each preactivate -> activation layer\n",
        "            for layer in range(2, len(self.dims)):\n",
        "                nabla_h = np.dot(self.weights[-layer+1], nabla_a)\n",
        "                nabla_a = nabla_h * self.activate_grad(a[-layer])\n",
        "                \n",
        "                nabla_b[-layer] = nabla_a\n",
        "                nabla_w[-layer] = np.outer(h[-layer-1], nabla_a)\n",
        "                \n",
        "            n_w = [x+y for x,y in zip(n_w, nabla_w)]\n",
        "            n_b = [x+y for x,y in zip(n_b, nabla_b)]\n",
        "            \n",
        "        n_w = [x / labels.shape[0] for x in n_w]\n",
        "        n_b = [x / labels.shape[0] for x in n_b]\n",
        "            \n",
        "        return (n_w,n_b)\n",
        "   \n",
        "        \n",
        "    def update(self,grads,lr):\n",
        "        grads_w, grads_b = grads\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] -= lr * grads_w[i]\n",
        "            self.biases[i] -= lr * grads_b[i]\n",
        "            \n",
        "\n",
        "    def train(self, data, epochs, batch_size, lr, lambd=0.0, test_data=None):\n",
        "        for ep in range(1, epochs+1):\n",
        "            print('Epoch',ep,':')\n",
        "            for (batch_x, batch_y) in data_iter(data, batch_size):\n",
        "                y_pred, ls, cache = self.forward(batch_x, batch_y)\n",
        "                grads = self.backward(cache, batch_y, ls)\n",
        "                self.update(grads, lr)\n",
        "            if test_data:\n",
        "                print('Epoch %i acc: %i.' % (ep,test(test_data)))\n",
        "\n",
        "                \n",
        "    def test(self, data):\n",
        "        x, y = data\n",
        "        outputs, _, _ = forward(x)\n",
        "        return accuracy(outputs, y)\n",
        "        \n",
        "    \n",
        "    def activate_grad(self,inputs):\n",
        "        if self.activate == 'relu':\n",
        "            inputs[inputs > 0] = 1\n",
        "            inputs[inputs < 0] = 0\n",
        "            return inputs\n",
        "        elif self.activate == 'sigmiod':\n",
        "            return self.activation(inputs) * (1 - self.activation(inputs))\n",
        "        \n",
        "        \n",
        "    def softmax(self,inputs):\n",
        "        inputs = inputs - np.max(inputs, axis=1).reshape(inputs.shape[0],1)\n",
        "        outputs = np.exp(inputs)\n",
        "        return outputs / (np.sum(outputs, axis=1).reshape(inputs.shape[0],1))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ULc4DpLmPJ3L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nn = NN(hidden_dims=[1024,512],n_hidden=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q_e1swpLPNFY",
        "colab_type": "code",
        "outputId": "7d0d39f6-5810-4bc5-b6e8-a7894e44a1ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "cell_type": "code",
      "source": [
        "nn.train((X_train,y_train), 10, 200, 0.01, test_data=(X_train,y_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 :\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-148-151a2295617e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-146-de37c0df07d6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data, epochs, batch_size, lr, lambd, test_data)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                 \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-146-de37c0df07d6>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, cache, labels, lss)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# for each preactivate -> activation layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0mnabla_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnabla_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                 \u001b[0mnabla_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnabla_h\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivate_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "BZ7uspNLn3Ss",
        "colab_type": "code",
        "outputId": "058a3084-f805-4721-ec5a-96941d4e144b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(784, 1024) (1024,)\n",
            "(1024, 2048) (2048,)\n",
            "(2048, 10) (10,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3rZ6REJwva-1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}